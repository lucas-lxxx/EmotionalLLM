# 1.26 online meeting

## 1. 项目概述与当前进度

**核心任务**：对白盒可访问的 OpenS2S 做音频扰动攻击，在几乎不破坏内容/语义的前提下，定向让模型输出目标情绪标签。

**当前进度**：

- 攻击闭环已跑通：输入音频 → 优化扰动 → 输出指定情绪
- 在 prompt 为"直接输出音频情绪标签"的设置下，攻击成功率约 **80%**
- 完成了"语义 vs 韵律（音频内冲突）"的 probe 实验，获得层级结构性现象
- 正在推进机理分析，将"可读性"升级为"因果仲裁"证据

---

## 2. 音频与 Prompt 冲突的机理分析

### 2.1 研究问题

当输入音频本身呈现情绪 A，但指令/文本 prompt 强行要求输出情绪 T（T≠A）时：

- 模型的输出更偏向谁？
- 这种偏向在层级上是"早就决定"还是"越到后面越固化"？
- 通过干预中间层激活能否改变最终输出（因果证据）？

### 2.2 采用的方法

- **Logit lens / decision tracing**：逐层把 hidden states 投到 vocab 上，比较情绪标签的 logit 差随层数变化，找到"仲裁趋势/固化层"
- **Activation patching**：在某层把"指令 token 区域"或"音频区域"的激活替换成另一个条件的激活，观察输出是否翻转（flip-rate vs layer）

> 这套方法是后续把"probe 可读性"升级成"因果仲裁"的核心工具，计划复用到 OpenS2S 的"语义 vs 韵律"冲突分析上。
> 

---

## 3. 我的实验：音频内冲突（语义情绪 vs 韵律情绪）

### 3.1 实验目的

研究"音频内部的两种情绪线索"：

- **语义情绪（semantic）**：文本内容本身表达的情绪
- **韵律情绪（prosody）**：同一句话用不同语调/情绪风格说出来

核心问题：

> 在模型不同层里，哪一类信息更强、更可读？尤其在语义与韵律冲突时，模型表征更偏向哪一边？
> 

### 3.2 数据与设置

- 5 类情绪：neutral / happy / sad / angry / surprised
- 50 条文本（每类 10 条），每条文本生成 5 种韵律版本 → 理论 250 条，实际可用 247 条
- 冲突样本 197；一致样本 50
- Prompt 固定为中性："What is the emotion of this audio? Answer with exactly one word: neutral, happy, sad, angry, surprised." 不在prompt里引导模型听谁的，只让他正常判断。

由于opens2s不是每过一层就判断，而是很多层transformer逐层处理。所以对每一条都跑一次forward，然后0-35层每层的hidden states提取出来，取出audio span，做pooling得到向量，再用probe看这一层具体是偏向哪里。

### 3.3 核心指标

对每层训练两个 probe：

- 预测 `text_emotion`（语义）
- 预测 `prosody_emotion`（韵律）

定义主导性：

- **D(layer) = Acc_prosody − Acc_semantic**
- D>0 视为韵律更"可读"；D<0 视为语义更"可读"

---

## 4. 本轮结果总结

### 4.1 重要数字

- 整体：overall dominance = prosody，平均 D ≈ 0.0526
- 韵律最强：layer 0 的 prosody_acc ≈ 0.842
- 语义最强：layer 27 的 semantic_acc ≈ 0.830
- 韵律主导峰值：layer 5，D ≈ 0.2146（冲突子集 D_conf ≈ 0.2182）
- 语义占优最明显：layer 26，D ≈ −0.0414

![](conflict_curves.png)

### 4.2 层级结构

根据 D(layer) 曲线：

- 早层（0–14）显著韵律主导（连续 D>0；早层平均 D≈0.146）
- 中层（约 12–23）接近融合态（平均 D≈0，且在 14–15 层出现符号翻转）
- 26–28 层出现小幅语义占优窗口
- 晚层（29–34）韵律再次回潮（D>0）

冲突子集（semantic≠prosody）上整体也更偏韵律：

- avg semantic_acc ≈ 0.7299
- avg prosody_acc ≈ 0.7895
- avg dominance ≈ 0.0596

![](dominance_curve.png)

### 4.3 结果分析

实验结果显示韵律的权重微弱高于语义，说明 OpenS2S 在训练中确实注重了模型感知情绪的功能。模型能"听出来"情绪，只不过最后判断的时候没有完全考虑进去。

---

## 5. 下一步计划

### 5.1 把"可读性"升级为"因果仲裁"

已有 D(layer) 的结构性现象，需要补充从"模型听得出来"到"模型判断出来"的证据：

- **Decision-level tracing / logit lens**：在"情绪标签输出位置"逐层看模型更倾向输出 semantic 还是 prosody（logit margin 曲线），验证决策轨迹是否与 D(layer) 对齐

### 5.2 机制引导攻击

研究清楚机理后，方法论可为机理量身打造。例如按照韵律和情绪的不同主导区间，设计不同的损失函数。

---

## 6. 后续工作

### 6.1 Logit-lens（逐层看"模型倾向输出哪种情绪"）

1. 取出这一层"与输出情绪标签相关"的那一小段表示（比如输出情绪 token 对应位置的 hidden state）
2. 假装"现在就让模型直接出答案"：把这个 hidden state 通过模型自己的输出头（lm head / classifier head）变成"各个情绪标签的分数（logits）"
3. 看这些分数里各情绪标签的分数大小
4. 对每一层都做一遍，画曲线

### 6.2 Activation Patching（因果实验：换掉某层"零件"，看输出会不会跟着变）

- A：同一句文本，用 sad 语义，但韵律是 happy（开心语调）
- B：同一句文本，韵律改成 sad（伤心语调）
- A 和 B 的语义一样，区别基本就是韵律

探究有没有可能是某一层在操纵最后的输出，把那一层换掉输出就变。

### 6.3 多模型验证

[https://huggingface.co/tsinghua-ee/SALMONN-7B](https://huggingface.co/tsinghua-ee/SALMONN-7B)