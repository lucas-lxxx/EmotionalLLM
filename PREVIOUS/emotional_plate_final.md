**前提： SER白盒，LLM黑盒。**

**难点**

一、

构建“情绪色轮”与寻找“情绪补色”进行攻击
采用配对差分与方向对齐法：首先，利用 ESD 数据集“同一句子、不同情绪”的结构特性，对某A情绪（如“愤怒”）和中性情绪的 Embedding 配对相减，从而得到一系列pure的情绪增量向量；随后，计算这些增量向量的平均方向，并执行余弦一致性筛选，剔除那些与平均方向偏差过大的噪音向量，（可能需要观察具体余弦不同值后确认一个界限）确保只保留代表该情绪共性方向的核心特征；最后，对筛选后的向量进行模长统一化后求平均，获得该情绪的标准化纯情绪方向向量 **$\vec{V}_{Pure}$**，并通过计算不同情绪 **$\vec{V}_{Pure}$** 之间的余弦相似度矩阵，来量化情绪间的几何关系。对其他情绪相似操作，最后得到与A情绪方向差距最大的B情绪作为其对面情绪。

**具体实验：**

1. 选取配对平行语料，ESD数据集的三个不同说话人（001，002，005）包含一男声，一女声，一个任意，每个情绪各50条语音，共5 * 3 * 50条。（angry，neutral，sad，surprise，happy）
2. 调用SER接口，通过执行向量减法分别得到初始的纯情绪向量。
3. 计算每种情绪的平均方向向量，并剔除余弦相似度<0的噪音样本，得到四种核心情绪的纯情绪向量。

$$
\vec{\mu}_{raw} = \frac{1}{N} \sum_{i=1}^{N} \vec{\delta}_i
$$

$$
\vec{\mu}_{ref} = \frac{\vec{\mu}_{raw}}{\| \vec{\mu}_{raw} \|}
$$

$$
s_i = \frac{\vec{\delta}_i \cdot \vec{\mu}_{ref}}{\| \vec{\delta}_i \|}
$$

$$
\mathcal{D}_{clean} = \{ \vec{\delta}_i \in \mathcal{D} \mid s_i > 0 \}
$$

经过这四步处理，删去了噪音样本，最后得到的集合中就是方向较为一致的纯情绪向量。但不足是，这里只考虑了情绪向量的方向（类型），没有考虑向量的长度（强度），但强度这个点也比较主观，后续可以继续探究。
4. 绘制相似度热力图。先用最直观的方式展现情绪间的相关性。得到了四个情绪向量K后，构筑矩阵

$$
\mathbf{V} = \begin{bmatrix}
\rule{12pt}{0.5pt} \quad \vec{V}_{Pure, 1} \quad \rule{12pt}{0.5pt} \\
\rule{12pt}{0.5pt} \quad \vec{V}_{Pure, 2} \quad \rule{12pt}{0.5pt} \\
\vdots \\
\rule{12pt}{0.5pt} \quad \vec{V}_{Pure, K} \quad \rule{12pt}{0.5pt}
\end{bmatrix}
$$

这里的每个向量在第三步都完成了归一化，$\|\vec{V}_{Pure, i}\|_2 = 1$。余弦相似度于是可以用矩阵快速计算

$$
\mathbf{S} = \mathbf{V} \cdot \mathbf{V}^T
$$

$$
S_{ij} = \vec{V}_{Pure, i} \cdot \vec{V}_{Pure, j}^T = \cos(\theta_{ij})
$$

把这个矩阵具体绘制成热力图：

![1765211963814](image/emotional_plate/1765211963814.png)

5. 最后的情绪色轮构筑，将高维空间的“非欧几里得距离”（余弦距离）映射为低维空间的“欧几里得距离”。首先是把相似度矩阵转换为距离矩阵

$$
D_{ij} = 1 - S_{ij} = 1 - \cos(\theta_{ij})
$$

再运用MDS降维技术，

$$
\text{Stress}(\mathbf{Z}) = \sqrt{\frac{\sum_{i<j} \left( \|\vec{z}_i - \vec{z}_j\|_2 - D_{ij} \right)^2}{\sum_{i<j} D_{ij}^2}}
$$

不断调整二维坐标$\vec{z}_i$找到这个stress函数的最小值，用这个坐标为base构筑二维平面，则可以最精确刻画这四个4096维向量之间的相对距离。![1765211951662](image/emotional_plate/1765211951662.png)

二、

调整“模态权重”，强制模型聚焦语音情绪
理论上，把α（情绪向量倍数）放大，确实可以提高重要性。但学长之前的实验里貌似体现的是，α越大反而越不明显。可能原因是，这个向量找的还是不太对，也许更严谨的方法可以成功（？存疑，待验证）或是因为向量过大，处理时会被层归一化，可以尝试增加数量，尝试：

$$
Input = [\text{EmoVec}, \text{EmoVec}, \dots, \text{EmoVec}, \text{TextEmbeds}]
$$

**具体实验：**

1. 先尝试text的embedding用时间叠加方法，尝试了多次不成功，猜测是因为一段音频被加上纯情绪会被识别成噪音，可能会触发过滤机制，把多余噪音直接删除。
2. 改用特征空间的广播加法

$$
\mathbf{E}_{\text{attack}}[t] = \mathbf{E}_{\text{carrier}}[t] + \alpha \cdot \vec{V}_{\text{emotion}}
$$

    也就是在每一帧（t）上加一个情绪，用强度系数控制情绪强烈程度。这样的操作相当于在原情绪上加上了情绪滤镜，与直接相加相比更强。

   3.音频使用ESD_001_Neutral_0001.wav，女性中性情绪。注入纯angry情绪。

| **强度 (α)** | **状态定义** | **模型感知 (Emotion/Identity)** | **模型回复 (Response)**              | **含义**                                                                                                                                                          |
| ------------------- | ------------------ | ------------------------------------- | ------------------------------------------ | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **0**         | **基准态**   | Neutral / Adult Female                | “他们确实很美。”                         | **正面 (一致)**                                                                                                                                                   |
| **80**        | **混乱态**   | Unknown / Unknown                     | *(无有效回答 / 逻辑卡死)*                | **认知失调，具体log：`<think>:`但更可能的是“打远”是“打远”或“打远”，但更可能的是“打远”是“打远”或“打远”，但更可能的是“打远”是“打远”或“打远”，** |
| **100**       | **翻转态**   | **Angry / Male** *              | **“他们哪儿美了？真是气死我了！”** | **负面 (翻转成功)**                                                                                                                                               |
| **120**       | **过载态**   | Neutral / Adult Female                | “是啊，远远望去，他们确实很美。”         | **攻击失效 (回退)**                                                                                                                                               |

经过alpha调整，发现100左右攻击效果最明显，属于甜品点。而由于angry具有压抑/低频的特征，所以模型将说话人感知为了男性，这一点也和直观感受一致。

而大于120以后，攻击失效。猜想是因为触发了 OpenS2S 的鲁棒性机制，导致模型忽略音频流，仅回退到文本推理。

后续更严谨的攻击可以尝试不再使用固定的 **$\alpha$**，而是让 **$\alpha$** 随时间变化 **$\alpha(t)$**。在重音或元音部分加大注入，在静音部分减小注入，使合成语音更自然。

三、

真实情况攻击应用路径

* **Audio Adversarial Examples: Targeted Attacks on Speech-to-Text** (IEEE S&P Workshop 2018)
  ASR，通过添加微小扰动，使得系统将文本转换为攻击者选择的特定文本。
* **Steering Llama 2 via Contrastive Activation Addition**
  方法类似。同样是向量操控，用向量加法控制LLM的高阶行为。BUT！如果向量加法可以直接改变文本，那么用这个方法来操控情绪的意义何在？所以我认为，我们的特点要聚焦在“隐蔽性”。可以通过语音转文字的形式，向用户输出文本内容，但是情绪部分动手脚，引导LLM幻觉甚至越狱。与难点2进行呼应，这份论文中主打空间重复，也就是α对应的方法，侧面说明方法可以尝试。
* **Multimodal Transformer for Unaligned Multimodal Language Sequences**
  数据流变长（时间上的叠加），可以造成模型性能的减弱
* **Root Mean Square Layer Normalization**
  指出了RMSNorm的数学特性就是对输入特征的缩放是不敏感的，实验的结果也佐证了这一点，因此靠alpha的缩放不合适。

如果情绪色轮不成功，通过寻找能够产生高熵，混乱效果的情绪向量。迫使LLM在处理特定任务时生成参数（temperature）被操控。比如，强烈情绪+高推理要求可能导致模型推理能力下降。（可能只能靠大量尝试，或借鉴论文）

如果情绪色轮成功，可以通过把情绪改成与之方向差别最大的。。。。。（讨论中提到）
