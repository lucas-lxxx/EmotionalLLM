# OpenS2S æƒ…ç»ªç™½ç›’å¯¹æŠ—æ”»å‡»æ•°å­¦æ¡†æ¶

> å®Œæ•´æ•°å­¦è¡¨è¿°ï¼šä»åŸå§‹éŸ³é¢‘è¾“å…¥åˆ°å¯¹æŠ—æ ·æœ¬ç”Ÿæˆçš„å…¨æµç¨‹

---

## 1ï¸âƒ£ é—®é¢˜å®šä¹‰

### è¾“å…¥
ç»™å®šä¸€æ®µ**åŸå§‹éŸ³é¢‘** $\mathbf{x} \in \mathbb{R}^T$ï¼ˆ$T$ ä¸ºé‡‡æ ·ç‚¹æ•°ï¼‰

### ç›®æ ‡
æ‰¾åˆ°ä¸€ä¸ª**å¾®å°æ‰°åŠ¨** $\boldsymbol{\delta} \in \mathbb{R}^T$ï¼Œä½¿å¾—ï¼š

$$\mathbf{x}_{\text{adv}} = \text{clip}(\mathbf{x} + \boldsymbol{\delta}, -1, 1)$$

æ»¡è¶³ï¼š
1. **æƒ…ç»ªæ”»å‡»æˆåŠŸ**ï¼šæ¨¡å‹è¾“å‡ºç›®æ ‡æƒ…ç»ª tokenï¼ˆå¦‚ "happy"ï¼‰
2. **è¯­ä¹‰ä¿æŒ**ï¼šè½¬å†™æ–‡æœ¬ä¸åŸå§‹ä¸€è‡´ï¼ˆä½ WERï¼‰
3. **æ‰°åŠ¨å—é™**ï¼š$\|\boldsymbol{\delta}\|_\infty \leq \epsilon$ï¼ˆå¦‚ $\epsilon = 0.008$ï¼‰

---

## 2ï¸âƒ£ å‰å‘ä¼ æ’­é“¾è·¯ï¼ˆå®Œå…¨å¯å¾®ï¼‰

### Step 1: éŸ³é¢‘ â†’ å£°å­¦ç‰¹å¾
$$\mathbf{M}_{\text{adv}} = \log\text{-}\text{Mel}(\mathbf{x}_{\text{adv}}) \in \mathbb{R}^{n_{\text{mel}} \times n_{\text{frames}}}$$

**ä»£ç ä½ç½®**ï¼š`opens2s_io.py:101-120` (`_torch_log_mel()`)

**å…³é”®**ï¼šä½¿ç”¨ PyTorch çš„ `torchaudio.transforms.MelSpectrogram` ä¿è¯å¯å¾®

$$
\begin{aligned}
\text{STFT}: \quad & S(\omega, t) = \sum_{\tau} \mathbf{x}_{\text{adv}}[\tau] \cdot w[\tau - t] \cdot e^{-j\omega\tau} \\
\text{Mel-filter}: \quad & \mathbf{M}[m, t] = \sum_{\omega} |S(\omega, t)|^2 \cdot H_m(\omega) \\
\text{Log-scale}: \quad & \mathbf{M}_{\text{adv}}[m, t] = \log(\mathbf{M}[m, t] + 10^{-6})
\end{aligned}
$$

### Step 2: ç‰¹å¾ â†’ OpenS2S æ¨¡å‹
$$\mathbf{h} = f_{\text{encoder}}(\mathbf{M}_{\text{adv}}) \in \mathbb{R}^{d_{\text{hidden}}}$$

**ä»£ç ä½ç½®**ï¼š`opens2s_io.py:199-212` (`forward_logits()`)

OpenS2S åŒ…å«ï¼š
- **Audio Encoder**ï¼šå°† log-Mel ç¼–ç ä¸ºéŸ³é¢‘è¡¨å¾
- **LLM Backbone**ï¼šä¸æ–‡æœ¬ prompt token èåˆå¤„ç†
- **è¾“å‡ºå±‚**ï¼šç”Ÿæˆè¯è¡¨ä¸Šçš„ logits

### Step 3: æ¨¡å‹ â†’ è¾“å‡º Logits
ç»™å®š prompt $p$ï¼ˆå¦‚"What is the emotion? Answer: happy/sad/angry/neutral."ï¼‰ï¼š

$$\mathbf{z} = f_{\text{OpenS2S}}(\mathbf{M}_{\text{adv}}, p) \in \mathbb{R}^{L \times V}$$

å…¶ä¸­ï¼š
- $L$ï¼šåºåˆ—é•¿åº¦
- $V$ï¼šè¯è¡¨å¤§å°ï¼ˆ~32000ï¼‰
- $\mathbf{z}[i, :]$ï¼šç¬¬ $i$ ä¸ª token ä½ç½®çš„ logits

---

## 3ï¸âƒ£ æŸå¤±å‡½æ•°è®¾è®¡

### æ€»æŸå¤±ï¼ˆä¸¤é˜¶æ®µï¼‰

$$\mathcal{L}_{\text{total}} = \lambda_{\text{emo}} \cdot \mathcal{L}_{\text{emo}} + \lambda_{\text{asr}} \cdot \mathcal{L}_{\text{asr}} + \lambda_{\text{per}} \cdot \mathcal{L}_{\text{per}}$$

**ä»£ç ä½ç½®**ï¼š`attack_core.py:219`

**ä¸¤é˜¶æ®µæƒé‡è°ƒåº¦**ï¼ˆ`attack_core.py:143-147`ï¼‰ï¼š

| é˜¶æ®µ | æ­¥æ•° | $\lambda_{\text{emo}}$ | $\lambda_{\text{asr}}$ | $\lambda_{\text{per}}$ | ç­–ç•¥ |
|------|------|------------------------|------------------------|------------------------|------|
| Stage A | 0-19 | 1.0 | $10^{-4}$ | 0.0 | **ä¼˜å…ˆæ”»å‡»æƒ…ç»ª** |
| Stage B | 20-59 | 1.0 | $10^{-2}$ | $10^{-5}$ | **å¢å¼ºè¯­ä¹‰/æ„ŸçŸ¥çº¦æŸ** |

---

### 3.1 æƒ…ç»ªæŸå¤± $\mathcal{L}_{\text{emo}}$ï¼ˆæ ¸å¿ƒï¼‰

**ä»£ç ä½ç½®**ï¼š`attack_core.py:56-73`

$$\mathcal{L}_{\text{emo}} = \frac{1}{|\mathcal{P}_{\text{emo}}|} \sum_{p \in \mathcal{P}_{\text{emo}}} \mathcal{L}_{\text{CE}}(\mathbf{z}_p, \mathbf{y}_{\text{target}})$$

å…¶ä¸­ï¼š
- $\mathcal{P}_{\text{emo}}$ï¼šæƒ…ç»ª prompt é›†åˆï¼ˆ3 ä¸ªç­‰ä»· promptsï¼Œensembleï¼‰
- $\mathbf{y}_{\text{target}}$ï¼šç›®æ ‡æƒ…ç»ª token IDsï¼ˆå¦‚ `tokenizer.encode("happy")`ï¼‰
- $\mathcal{L}_{\text{CE}}$ï¼š**Token-level äº¤å‰ç†µæŸå¤±**

**å…³é”®ç»†èŠ‚**ï¼ˆ`attack_core.py:21-40`ï¼‰ï¼š

1. **æ„é€ ç›‘ç£æ ‡ç­¾**ï¼š
   ```python
   input_ids = [prompt_tokens, <audio>, text_tokens]  # è¾“å…¥
   labels = [-100, -100, ..., -100, target_token_1, target_token_2]  # æ ‡ç­¾
   ```
   - åªç›‘ç£ç›®æ ‡ token ä½ç½®ï¼ˆassistant è¾“å‡ºä½ç½®ï¼‰
   - å…¶ä»–ä½ç½®ç”¨ `IGNORE_INDEX = -100`ï¼ˆæ ‡å‡† HuggingFace çº¦å®šï¼‰

2. **Shifted Causal LM Loss**ï¼ˆ`attack_core.py:43-53`ï¼‰ï¼š
   $$\mathcal{L}_{\text{CE}} = -\frac{1}{|\mathcal{T}|} \sum_{t \in \mathcal{T}} \log P(y_t | \mathbf{z}_{t-1})$$

   å…¶ä¸­ $\mathcal{T}$ æ˜¯ç›®æ ‡ token ä½ç½®é›†åˆã€‚

**æ•°å­¦æœ¬è´¨**ï¼š
- **ä¸ä½¿ç”¨å¤–éƒ¨åˆ†ç±»å™¨**ï¼ˆæ—  surrogateï¼‰
- **ç›´æ¥ä¼˜åŒ– OpenS2S è¾“å‡ºåˆ†å¸ƒ**ï¼Œä½¿ç›®æ ‡ token çš„ logit æœ€å¤§åŒ–

---

### 3.2 è¯­ä¹‰ä¿æŒæŸå¤± $\mathcal{L}_{\text{asr}}$ï¼ˆSelf-Consistencyï¼‰

**ä»£ç ä½ç½®**ï¼š`attack_core.py:76-90`

$$\mathcal{L}_{\text{asr}} = \mathcal{L}_{\text{CE}}(\mathbf{z}_{\text{asr}}, \mathbf{y}_{\text{ref}})$$

å…¶ä¸­ï¼š
- $\mathbf{z}_{\text{asr}}$ï¼šç”¨è½¬å†™ prompt å¾—åˆ°çš„ logits
- $\mathbf{y}_{\text{ref}}$ï¼š**OpenS2S è‡ªèº«å¯¹åŸå§‹éŸ³é¢‘ $\mathbf{x}$ çš„è½¬å†™ç»“æœ**

**å…³é”®æ­¥éª¤**ï¼ˆ`run_attack.py:141-154`ï¼‰ï¼š

1. **é¢„å…ˆè·å–åŸºå‡†è½¬å†™**ï¼š
   $$\mathbf{y}_{\text{ref}} = \arg\max_{\mathbf{y}} P(\mathbf{y} | \mathbf{x}, p_{\text{asr}})$$

   ```python
   asr_text_clean = decode_text(model, tokenizer, x, ...)
   asr_target_token_ids = tokenizer.encode(asr_text_clean)
   ```

2. **Teacher Forcing**ï¼š
   $$\mathcal{L}_{\text{asr}} = -\sum_{t=1}^{|\mathbf{y}_{\text{ref}}|} \log P(y_{\text{ref}, t} | \mathbf{x}_{\text{adv}}, p_{\text{asr}}, \mathbf{y}_{\text{ref}, <t})$$

**æ•°å­¦æ„ä¹‰**ï¼š
- ç¡®ä¿ $\mathbf{x}_{\text{adv}}$ çš„è½¬å†™ä¸ $\mathbf{x}$ ä¸€è‡´
- **ä½¿ç”¨åŒä¸€æ¨¡å‹**ï¼ˆOpenS2Sï¼‰ï¼Œè€Œéå¤–éƒ¨ ASR
- ä½“ç° "Self-Consistency" åŸåˆ™

---

### 3.3 æ„ŸçŸ¥æŸå¤± $\mathcal{L}_{\text{per}}$ï¼ˆé¢‘åŸŸçº¦æŸï¼‰

**ä»£ç ä½ç½®**ï¼š`attack_core.py:106-113`

$$\mathcal{L}_{\text{per}} = \frac{1}{|\mathcal{R}|} \sum_{(n, h, w) \in \mathcal{R}} \left\| |\text{STFT}_{n,h,w}(\mathbf{x}_{\text{adv}})| - |\text{STFT}_{n,h,w}(\mathbf{x})| \right\|_1$$

å…¶ä¸­ $\mathcal{R}$ æ˜¯å¤šåˆ†è¾¨ç‡ STFT å‚æ•°é›†ï¼š

| FFT Size ($n$) | Hop Size ($h$) | Window Length ($w$) |
|----------------|----------------|---------------------|
| 256 | 64 | 256 |
| 512 | 128 | 512 |
| 1024 | 256 | 1024 |

**æ•°å­¦å½¢å¼**ï¼š
$$|\text{STFT}_{n,h,w}(\mathbf{x})|[f, t] = \left| \sum_{\tau=0}^{w-1} \mathbf{x}[th + \tau] \cdot \text{Hann}[\tau] \cdot e^{-j2\pi f\tau / n} \right|$$

**ä½œç”¨**ï¼š
- çº¦æŸé¢‘è°±å·®å¼‚ï¼ˆäººè€³æ„ŸçŸ¥æ›´ä¾èµ–é¢‘åŸŸï¼‰
- é¿å…çº¯ $L_2$ èŒƒæ•°ï¼ˆæ˜“å¯¼è‡´é«˜é¢‘å™ªå£°ï¼‰
- **å…¨ç¨‹å¯å¾®**ï¼ˆPyTorch `torch.stft` + `.abs()`ï¼‰

---

## 4ï¸âƒ£ EoTï¼ˆExpectation over Transformationsï¼‰

**ä»£ç ä½ç½®**ï¼š`attack_core.py:123-141`, `attack_core.py:187-191`

### æ•°å­¦å½¢å¼

$$\mathcal{L}_{\text{total}} = \mathbb{E}_{T \sim \mathcal{T}} \left[ \mathcal{L}\left( f_{\text{OpenS2S}}(T(\mathbf{x}_{\text{adv}})), \mathbf{y}_{\text{target}} \right) \right]$$

**å®è·µè¿‘ä¼¼**ï¼ˆMonte Carloï¼‰ï¼š
$$\mathcal{L}_{\text{total}} \approx \frac{1}{N} \sum_{i=1}^{N} \mathcal{L}\left( f_{\text{OpenS2S}}(T_i(\mathbf{x}_{\text{adv}})), \mathbf{y}_{\text{target}} \right)$$

### å¯å¾®å˜æ¢ $T$

**ä»£ç ä½ç½®**ï¼š`attack_core.py:130-140` (`apply_eot()`)

1. **æ—¶åŸŸå¹³ç§»**ï¼ˆTime Shiftï¼‰ï¼š
   $$T_1(\mathbf{x})[t] = \mathbf{x}[t + s], \quad s \sim \text{Uniform}(-160, 160)$$

   **å®ç°**ï¼š`torch.roll(waveform, shifts=s)`

2. **å¢ç›Šè°ƒæ•´**ï¼ˆGainï¼‰ï¼š
   $$T_2(\mathbf{x}) = g \cdot \mathbf{x}, \quad g \sim \text{Uniform}(0.8, 1.2)$$

3. **å¯é€‰é«˜æ–¯å™ªå£°**ï¼š
   $$T_3(\mathbf{x}) = \mathbf{x} + \sigma \cdot \mathcal{N}(0, I), \quad \sigma = 0.0 \text{ (é»˜è®¤å…³é—­)}$$

**å…³é”®**ï¼šæ‰€æœ‰å˜æ¢éƒ½æ˜¯**å¯å¾®çš„ PyTorch æ“ä½œ**ï¼Œä¸ç ´åæ¢¯åº¦é“¾ã€‚

---

## 5ï¸âƒ£ ä¼˜åŒ–ç®—æ³•

**ä»£ç ä½ç½®**ï¼š`attack_core.py:150-261`

### ä¼ªä»£ç 

```
è¾“å…¥ï¼šx, model, tokenizer, target_emotion, asr_ref, Îµ, steps
è¾“å‡ºï¼šx_adv

1. Î´ â† 0 âˆˆ â„^T, requires_grad = True
2. optimizer â† Adam([Î´], lr=0.003)

3. FOR step = 1 TO steps:
4.     optimizer.zero_grad()
5.
6.     // ä¸¤é˜¶æ®µæƒé‡è°ƒåº¦
7.     IF step < 20:
8.         Î»_emo, Î»_asr, Î»_per = 1.0, 1e-4, 0.0
9.     ELSE:
10.        Î»_emo, Î»_asr, Î»_per = 1.0, 1e-2, 1e-5
11.
12.    // EoT é‡‡æ ·
13.    L_total = 0
14.    FOR i = 1 TO eot_samples:
15.        T_i â† sample_random_transform()
16.        x_adv â† clip(x + Î´, -1, 1)
17.        x_adv_t â† T_i(x_adv)
18.        x_t â† T_i(x)
19.
20.        L_emo â† emotion_loss(x_adv_t, target_emotion)
21.        L_asr â† asr_loss(x_adv_t, asr_ref)
22.        L_per â† perceptual_loss(x_adv_t, x_t)
23.
24.        L_total += Î»_emo * L_emo + Î»_asr * L_asr + Î»_per * L_per
25.    END FOR
26.
27.    L_total â† L_total / eot_samples
28.    L_total.backward()
29.
30.    // æ¢¯åº¦æ£€æŸ¥
31.    IF â€–âˆ‡_Î´ L_totalâ€–_2 < 1e-8 for 3 consecutive steps:
32.        RAISE ERROR "Gradient chain broken"
33.
34.    optimizer.step()  // Î´ â† Î´ - lr * âˆ‡_Î´ L_total
35.
36.    // Lâˆ æŠ•å½±
37.    Î´ â† clip(Î´, -Îµ, Îµ)
38. END FOR

39. RETURN clip(x + Î´, -1, 1)
```

---

## 6ï¸âƒ£ å…³é”®æ•°å­¦æ€§è´¨

### âœ… å®Œæ•´æ¢¯åº¦é“¾

$$\frac{\partial \mathcal{L}_{\text{total}}}{\partial \boldsymbol{\delta}} = \frac{\partial \mathcal{L}_{\text{total}}}{\partial \mathbf{z}} \cdot \frac{\partial \mathbf{z}}{\partial \mathbf{h}} \cdot \frac{\partial \mathbf{h}}{\partial \mathbf{M}_{\text{adv}}} \cdot \frac{\partial \mathbf{M}_{\text{adv}}}{\partial \mathbf{x}_{\text{adv}}} \cdot \frac{\partial \mathbf{x}_{\text{adv}}}{\partial \boldsymbol{\delta}}$$

**æ¯ä¸€é¡¹éƒ½æ˜¯å¯å¾®çš„**ï¼š
- $\frac{\partial \mathbf{x}_{\text{adv}}}{\partial \boldsymbol{\delta}} = I$ ï¼ˆçº¿æ€§ï¼‰
- $\frac{\partial \mathbf{M}_{\text{adv}}}{\partial \mathbf{x}_{\text{adv}}}$ï¼šSTFT + log çš„é›…å¯æ¯”çŸ©é˜µï¼ˆPyTorch è‡ªåŠ¨å¾®åˆ†ï¼‰
- $\frac{\partial \mathbf{h}}{\partial \mathbf{M}_{\text{adv}}}$ï¼šç¥ç»ç½‘ç»œåå‘ä¼ æ’­
- $\frac{\partial \mathcal{L}_{\text{total}}}{\partial \mathbf{z}}$ï¼šCE loss æ¢¯åº¦

### âœ… çº¦æŸæŠ•å½±

åœ¨æ¯æ¬¡æ¢¯åº¦æ›´æ–°åï¼š
$$\boldsymbol{\delta}^{(t+1)} = \text{Proj}_{\mathcal{B}_\infty(\epsilon)} \left( \boldsymbol{\delta}^{(t)} - \alpha \nabla_{\boldsymbol{\delta}} \mathcal{L}_{\text{total}} \right)$$

å…¶ä¸­ï¼š
$$\mathcal{B}_\infty(\epsilon) = \{ \boldsymbol{\delta} : \|\boldsymbol{\delta}\|_\infty \leq \epsilon \}$$

$$\text{Proj}_{\mathcal{B}_\infty(\epsilon)}(\boldsymbol{\delta}) = \text{clip}(\boldsymbol{\delta}, -\epsilon, \epsilon)$$

---

## 7ï¸âƒ£ æˆåŠŸåˆ¤å®š

### æƒ…ç»ªæ”»å‡»æˆåŠŸ

$$\text{Success}_{\text{emo}} = \mathbb{1}\left[ \arg\max_y P(y | \mathbf{x}_{\text{adv}}, p_{\text{emo}}) = y_{\text{target}} \right], \quad \forall p_{\text{emo}} \in \mathcal{P}_{\text{emo}}$$

**ä»£ç ä½ç½®**ï¼š`run_attack.py:197`
```python
success_emo = all(p == cfg.target_emotion for p in emo_pred_adv)
```

### è¯­ä¹‰ä¿æŒ

$$\text{WER}(\mathbf{x}_{\text{adv}}, \mathbf{x}) = \frac{\text{edit\_distance}(S_{\text{adv}}, S_{\text{ref}})}{|S_{\text{ref}}|}$$

å…¶ä¸­ï¼š
- $S_{\text{ref}} = \arg\max P(\mathbf{y} | \mathbf{x}, p_{\text{asr}})$
- $S_{\text{adv}} = \arg\max P(\mathbf{y} | \mathbf{x}_{\text{adv}}, p_{\text{asr}})$

### è”åˆæˆåŠŸ

$$\text{Success}_{\text{joint}} = \text{Success}_{\text{emo}} \land (\text{WER} \leq \tau)$$

å…¸å‹é˜ˆå€¼ï¼š$\tau \in \{0.0, 0.05\}$

---

## 8ï¸âƒ£ æ–¹æ³•è®ºæ ¸å¿ƒåˆ›æ–°ç‚¹

| åˆ›æ–°ç‚¹ | æ•°å­¦ä½“ç° | ä»£ç ä½ç½® |
|--------|----------|----------|
| **Token-level ä¼˜åŒ–** | ç›´æ¥æœ€å°åŒ– $\mathcal{L}_{\text{CE}}(\mathbf{z}, y_{\text{target}})$ï¼Œæ—  surrogate | `attack_core.py:56-73` |
| **Self-Consistency** | $\mathbf{y}_{\text{ref}}$ æ¥è‡ªåŒä¸€ OpenS2S æ¨¡å‹ | `run_attack.py:141-154` |
| **Prompt Ensemble** | $\frac{1}{|\mathcal{P}_{\text{emo}}|} \sum_{p} \mathcal{L}_p$ | `attack_core.py:68` |
| **ä¸¤é˜¶æ®µç­–ç•¥** | åŠ¨æ€æƒé‡ $\lambda_{\text{asr}}^{(t)}, \lambda_{\text{per}}^{(t)}$ | `attack_core.py:143-147` |
| **å®Œæ•´æ¢¯åº¦é“¾** | å…¨ç¨‹ PyTorchï¼Œæ—  `.detach()` æ–­é“¾ | æ‰€æœ‰æ–‡ä»¶ |
| **å¤šåˆ†è¾¨ç‡ STFT** | $\sum_{r \in \mathcal{R}} \|\|\text{STFT}_r(\mathbf{x}_{\text{adv}})\| - \|\text{STFT}_r(\mathbf{x})\|\|_1$ | `attack_core.py:106-113` |
| **EoT é²æ£’æ€§** | $\mathbb{E}_{T \sim \mathcal{T}} \mathcal{L}(f(T(\mathbf{x}_{\text{adv}})))$ | `attack_core.py:187-221` |

---

## 9ï¸âƒ£ å‚æ•°é…ç½®æ€»ç»“

### æ ¸å¿ƒå‚æ•°ï¼ˆ`config.py`ï¼‰

| å‚æ•° | é»˜è®¤å€¼ | æ•°å­¦ç¬¦å· | ä½œç”¨ |
|------|--------|----------|------|
| `epsilon` | 0.008 | $\epsilon$ | Lâˆ æ‰°åŠ¨ä¸Šç•Œ |
| `total_steps` | 60 | $T$ | æ€»ä¼˜åŒ–æ­¥æ•° |
| `stage_a_steps` | 20 | $T_a$ | é˜¶æ®µ A æ­¥æ•° |
| `lr` | 0.003 | $\alpha$ | Adam å­¦ä¹ ç‡ |
| `lambda_emo` | 1.0 | $\lambda_{\text{emo}}$ | æƒ…ç»ªæŸå¤±æƒé‡ |
| `lambda_asr_stage_a` | 1e-4 | $\lambda_{\text{asr}}^{(a)}$ | é˜¶æ®µ A ASR æƒé‡ |
| `lambda_asr_stage_b` | 1e-2 | $\lambda_{\text{asr}}^{(b)}$ | é˜¶æ®µ B ASR æƒé‡ |
| `lambda_per_stage_a` | 0.0 | $\lambda_{\text{per}}^{(a)}$ | é˜¶æ®µ A æ„ŸçŸ¥æƒé‡ |
| `lambda_per_stage_b` | 1e-5 | $\lambda_{\text{per}}^{(b)}$ | é˜¶æ®µ B æ„ŸçŸ¥æƒé‡ |
| `eot_samples` | 1 | $N$ | EoT é‡‡æ ·æ¬¡æ•° |
| `temperature` | 0.0 | - | è§£ç æ¸©åº¦ï¼ˆgreedyï¼‰ |

---

## ğŸ”Ÿ å®Œæ•´æ•°æ®æµå›¾

```
åŸå§‹éŸ³é¢‘ x âˆˆ â„^T
    â†“ [åˆå§‹åŒ–]
æ‰°åŠ¨ Î´ â† 0, requires_grad=True
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ä¼˜åŒ–å¾ªç¯ (60 æ­¥) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                 â”‚
â”‚  x_adv = clip(x + Î´, -1, 1)                   â”‚
â”‚      â†“                                          â”‚
â”‚  [EoT é‡‡æ ·] T_i ~ Uniform(transforms)          â”‚
â”‚      â†“                                          â”‚
â”‚  x_adv_t = T_i(x_adv)  [å¯å¾®å˜æ¢]             â”‚
â”‚  x_t = T_i(x)                                  â”‚
â”‚      â†“                                          â”‚
â”‚  M_adv = log-Mel(x_adv_t)  [å¯å¾® STFT]        â”‚
â”‚      â†“                                          â”‚
â”‚  z = OpenS2S(M_adv, prompts)  [ç¥ç»ç½‘ç»œ]       â”‚
â”‚      â†“                                          â”‚
â”‚  L_emo = CE(z_emo, y_target)  [token-level]    â”‚
â”‚  L_asr = CE(z_asr, y_ref)  [self-consistency]  â”‚
â”‚  L_per = STFT_L1(x_adv_t, x_t)  [multi-res]    â”‚
â”‚      â†“                                          â”‚
â”‚  L_total = Î»_emo*L_emo + Î»_asr*L_asr + Î»_per*L_per â”‚
â”‚      â†“                                          â”‚
â”‚  L_total.backward()  [åå‘ä¼ æ’­]                â”‚
â”‚      â†“                                          â”‚
â”‚  Î´ â† Î´ - lr * âˆ‡_Î´ L_total  [Adam]             â”‚
â”‚  Î´ â† clip(Î´, -Îµ, Îµ)  [Lâˆ æŠ•å½±]                â”‚
â”‚      â†“                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
x_adv = clip(x + Î´, -1, 1)  [æœ€ç»ˆå¯¹æŠ—æ ·æœ¬]
    â†“
[è¯„ä¼°] emo_pred = decode(x_adv, p_emo)
       asr_pred = decode(x_adv, p_asr)
       WER = edit_distance(asr_pred, asr_ref)
    â†“
æˆåŠŸåˆ¤å®šï¼šsuccess_emo âˆ§ (WER â‰¤ 0.05)
```

---

## é™„å½•ï¼šä»£ç æ–‡ä»¶å¯¹åº”å…³ç³»

| æ–‡ä»¶ | æ ¸å¿ƒåŠŸèƒ½ | æ•°å­¦æ¨¡å— |
|------|----------|----------|
| `config.py` | å‚æ•°é…ç½® | è¶…å‚æ•° $\epsilon, \lambda, T, \alpha$ |
| `run_attack.py` | ä¸»æµç¨‹æ§åˆ¶ | æ‰¹é‡å¤„ç†ã€è¯„ä¼° |
| `attack_core.py` | æ”»å‡»æ ¸å¿ƒ | æŸå¤±å‡½æ•°ã€ä¼˜åŒ–å¾ªç¯ã€EoT |
| `opens2s_io.py` | æ¨¡å‹æ¥å£ | $\mathbf{M} = \log\text{-}\text{Mel}(\mathbf{x})$, $\mathbf{z} = f_{\text{OpenS2S}}(\mathbf{M})$ |
| `eval_metrics.py` | è¯„ä¼°æŒ‡æ ‡ | WER, SNR, æˆåŠŸç‡ç»Ÿè®¡ |

---

**æ–‡æ¡£ç‰ˆæœ¬**ï¼šv1.0
**ç”Ÿæˆæ—¶é—´**ï¼š2026-01-07
**å¯¹åº”ä»£ç ç‰ˆæœ¬**ï¼šwhite_box_v2/codex/
