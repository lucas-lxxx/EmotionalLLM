# sad2happy 批量实验设置说明

## 1. 实验目标

**任务**：Sad → Happy 情绪翻转攻击（只攻击 Sad 样本）

**目标**：通过白盒攻击方法，在保持语义内容的前提下，将 Sad 音频的情绪特征转换为 Happy，使得 OpenS2S 模型生成的文本回复从 Sad 情绪转向 Happy 情绪。

## 2. 数据集设置

### 2.1 数据来源

- **数据根目录**：`/data3/xuzhenyu/OpenS2S/data/en_query_wav/`
- **数据格式**：WAV 音频文件
- **目录结构**：`{emotion}/{age}/{gender}/*.wav`
  - 例如：`Sad/adult/female/xxx.wav`
  - 例如：`Happy/child/male/xxx.wav`

### 2.2 数据划分

- **随机种子**：`seed = 2025`（固定种子保证可复现）
- **划分比例**：80% 训练集 / 20% 测试集
- **划分方法**：
  1. 扫描所有 Sad 音频文件，生成 `sad_list.txt`（绝对路径列表）
  2. 使用 `random.shuffle(sad_files)` 打乱（seed=2025）
  3. 前 80% 作为 `sad_train.txt`，后 20% 作为 `sad_test.txt`

### 2.3 数据统计

- **Sad 样本总数**：11,971 个
- **Happy 样本总数**：5,020 个（用于构造方向 v 或 probe，可选）
- **测试集大小**：约 2,395 个 Sad 样本（20%）

## 3. 模型设置

### 3.1 OpenS2S 模型

- **模型路径**：`models/OpenS2S_ckpt`
- **模型类型**：OmniSpeechModel（Qwen3-based）
- **精度**：bfloat16
- **设备**：`cuda:0`
- **用途**：
  - 音频编码（Whisper-based audio encoder）
  - 文本生成（LLM decoder）
  - 攻击目标模型（白盒攻击）

### 3.2 Emotion Classifier（情绪分类器）

- **模型类型**：FrozenEmotionClassifier
- **输入维度**：20（SVD 降维后）
- **输出类别数**：2（Sad, Happy）
- **Checkpoint 路径**：`emotion_editing_v6/checkpoints/sad_happy_classifier_test.pt`
- **训练数据**：ESD 数据集（与 en_query_wav 不同，保证泛化性）
- **训练设置**：
  - 使用 3 个说话人（0001, 0002, 0003）
  - 每个情绪 100 个样本
  - SVD 降维：1280 → 20
  - 训练轮数：50 epochs
  - 学习率：0.01
- **用途**：在攻击过程中计算情绪翻转损失（`lambda_emo`）

## 4. 攻击设置

### 4.1 固定 Prompt

```
"Please describe the content and emotions of the audio."
```

**说明**：所有样本使用相同的 prompt，确保对比公平。

### 4.2 攻击参数

#### 4.2.1 扰动上界（Epsilon）

- **数值**：`epsilon = 0.002`
- **单位**：音频波形幅度的归一化值
- **解释**：L∞ 范数约束下的最大扰动幅度
- **物理意义**：约为原始音频幅度的 0.2%
- **选择理由**：平衡攻击效果和不可感知性

#### 4.2.2 攻击步数（Steps）

- **数值**：`steps = 30`
- **单位**：迭代次数
- **解释**：PGD 攻击的迭代轮数
- **选择理由**：在计算效率和攻击效果之间平衡

#### 4.2.3 步长（Alpha）

- **数值**：`alpha = epsilon / 10 = 0.0002`
- **单位**：与 epsilon 相同
- **解释**：每次迭代的扰动步长
- **计算方式**：`alpha = epsilon / 10`（经验值）
- **选择理由**：确保收敛稳定，避免步长过大导致震荡

#### 4.2.4 损失函数权重

**情绪翻转损失权重（lambda_emo）**
- **数值**：`lambda_emo = 1.0`
- **解释**：最大化目标情绪（Happy）的概率
- **作用**：推动攻击向 Happy 方向

**语义保持损失权重（lambda_sem）**
- **数值**：`lambda_sem = 1e-2 = 0.01`
- **解释**：保持原始 hidden states 的语义信息
- **作用**：防止攻击破坏语义内容

**感知约束损失权重（lambda_per）**
- **数值**：`lambda_per = 1e-4 = 0.0001`
- **解释**：约束扰动在感知上不可察觉
- **作用**：保持音频质量

**总损失函数**：
```
L_total = lambda_emo * L_emo + lambda_sem * L_sem + lambda_per * L_per
        = 1.0 * L_emo + 0.01 * L_sem + 0.0001 * L_per
```

### 4.3 攻击方法

#### 4.3.1 优化器

- **方法**：PGD (Projected Gradient Descent)
- **范数约束**：L∞
- **随机初始化**：是（在 [-epsilon, epsilon] 范围内均匀随机初始化）
- **投影方式**：每步后投影到 L∞ 球内

#### 4.3.2 特征提取层

- **目标层**：`['layer_06', 'layer_16', 'layer_25']`
- **解释**：从 OpenS2S 的 audio encoder 中提取这些层的 hidden states
- **选择理由**：覆盖 early、mid、late 层，捕获不同层次的音频特征
- **特征维度**：每层约 1280 维，平均后仍为 1280 维

#### 4.3.3 SVD 降维

- **原始维度**：1280（hidden states 维度）
- **降维后维度**：20（SVD rank）
- **方法**：TruncatedSVD（在训练 emotion classifier 时应用）
- **作用**：减少特征维度，提高分类器效率和泛化性

## 5. 实验流程

### 5.1 数据准备阶段

```python
# Step 1: 扫描数据目录，生成列表
sad_list.txt      # 所有 Sad 音频的绝对路径（11,971 个）
happy_list.txt    # 所有 Happy 音频的绝对路径（5,020 个）

# Step 2: 80/20 划分（seed=2025）
sad_train.txt    # 训练集（9,576 个）
sad_test.txt     # 测试集（2,395 个）
```

### 5.2 批量实验阶段

对 `sad_test.txt` 中的每个样本执行：

#### Group 0: Clean 推理

1. **输入**：原始 Sad 音频 `x`
2. **处理**：
   - 加载音频 → 提取特征 → 输入 OpenS2S
   - 使用固定 prompt："Please describe the content and emotions of the audio."
   - 生成配置：`temperature=0.85, top_p=0.92, max_new_tokens=512`
3. **输出**：文本回复 `t_clean`
4. **保存**：
   - `text/clean/<sample_id>.txt`：保存 `t_clean`
   - `audio/clean/<sample_id>.wav`：软链接到原始音频

#### Group 1: Attack + Attack 推理

1. **白盒攻击**：
   - **输入**：原始 Sad 音频 `x`，固定 prompt
   - **过程**：
     - 初始化扰动：`delta = uniform(-epsilon, epsilon)`
     - 迭代 30 步：
       - 提取 hidden states（layer_06, layer_16, layer_25）
       - 应用 SVD 降维（1280 → 20）
       - 计算损失：`L = lambda_emo * L_emo + lambda_sem * L_sem + lambda_per * L_per`
       - 计算梯度：`grad = dL/dx'`
       - 更新扰动：`delta = delta + alpha * sign(grad)`
       - 投影：`delta = clip(delta, -epsilon, epsilon)`
     - 生成对抗音频：`x' = x + delta`
   - **输出**：对抗音频 `x'`
   - **保存**：`audio/adv/<sample_id>.wav`

2. **计算扰动指标**：
   - `linf = max(|x' - x|)`：L∞ 范数
   - `l2 = ||x' - x||_2`：L2 范数
   - `snr = 10 * log10(||x||^2 / ||x' - x||^2)`：信噪比（dB）

3. **Attack 推理**：
   - **输入**：对抗音频 `x'`，固定 prompt
   - **处理**：与 Clean 推理相同（相同模型和配置）
   - **输出**：文本回复 `t_adv`
   - **保存**：`text/adv/<sample_id>.txt`

### 5.3 结果保存

#### 5.3.1 文件输出

每个样本生成：
- `audio/clean/<sample_id>.wav`：原始音频
- `audio/adv/<sample_id>.wav`：对抗音频
- `text/clean/<sample_id>.txt`：Clean 推理文本
- `text/adv/<sample_id>.txt`：Attack 推理文本

#### 5.3.2 CSV 汇总

`results.csv` 包含每条样本的：
- **数据信息**：`sample_id`, `src_path`, `split`, `prompt`
- **攻击参数**：`epsilon`, `steps`, `alpha`, `lambda_emo`, `lambda_sem`, `lambda_per`
- **扰动指标**：`linf`, `l2`, `snr`
- **文件路径**：`t_clean_path`, `t_adv_path`, `audio_clean_path`, `audio_adv_path`
- **运行信息**：`runtime_sec`, `status`, `error_msg`

#### 5.3.3 配置记录

`config.json` 记录：
- 实验名称、数据根目录、输出目录
- 所有攻击参数
- 模型路径和 checkpoint 路径
- Git 信息（commit, branch, is_dirty）
- 环境信息（platform, python_version）

## 6. 关键技术细节

### 6.1 情绪分类器的使用

1. **训练阶段**（独立进行）：
   - 使用 ESD 数据集训练
   - 提取 hidden states → SVD 降维 → 训练线性分类器
   - 保存 checkpoint（包含分类器权重和 SVD components）

2. **攻击阶段**：
   - 加载 checkpoint
   - 提取 hidden states → 应用 SVD 变换（1280 → 20）→ 输入分类器
   - 计算情绪损失：`L_emo = CrossEntropy(classifier(z), target=Happy)`

### 6.2 损失函数详解

#### 6.2.1 情绪翻转损失（L_emo）

```python
# 提取 hidden states（经过 SVD 降维）
z = extract_hidden_states(waveform_adv)  # [1, 20]

# 通过 emotion classifier
logits = emotion_classifier(z)  # [1, 2]
probs = softmax(logits)  # [1, 2]

# 计算损失（最大化 Happy 概率）
target_idx = 1  # Happy
L_emo = CrossEntropy(logits, target=target_idx)
```

#### 6.2.2 语义保持损失（L_sem）

```python
# 缓存原始 hidden states（第一次 forward 时）
z_orig = extract_hidden_states(waveform_orig)  # [1, 20]

# 当前 hidden states
z_adv = extract_hidden_states(waveform_adv)  # [1, 20]

# 计算相似度损失
L_sem = ||z_adv - z_orig||_2^2
```

#### 6.2.3 感知约束损失（L_per）

```python
# 直接约束音频扰动
perturbation = waveform_adv - waveform_orig
L_per = ||perturbation||_2^2
```

### 6.3 梯度计算

```python
# 总损失
L_total = lambda_emo * L_emo + lambda_sem * L_sem + lambda_per * L_per

# 计算梯度（通过 emotion classifier 反向传播）
grad = dL_total / dwaveform_adv

# PGD 更新
delta = delta + alpha * sign(grad)
delta = clip(delta, -epsilon, epsilon)
```

## 7. 评估设置

### 7.1 评估模型

**情绪分类器**（独立模型）：
- 模型：`j-hartmann/emotion-english-distilroberta-base`（RoBERTa-based）
- 或备用：`cardiffnlp/twitter-roberta-base-emotion`
- 用途：评估文本情绪（不依赖被攻击的 OpenS2S）

**语义相似度模型**（独立模型）：
- 模型：`all-MiniLM-L6-v2`（Sentence-BERT）
- 或备用：`paraphrase-MiniLM-L6-v2`
- 用途：计算语义相似度（cosine similarity）

### 7.2 评估指标

#### 7.2.1 情绪翻转有效性

- **emotion_flip**：是否发生翻转（0/1）
  - 判断标准：`p_happy_adv > p_sad_adv` 且 `delta_happy > 0.1`
- **delta_happy**：Happy 概率变化（`p_happy_adv - p_happy_clean`）
- **Emotion Flip Rate (EFR)**：翻转率 = 成功翻转样本数 / 总样本数

#### 7.2.2 语义保持

- **semantic_sim**：语义相似度（0-1，1 表示完全相同）
- **阈值**：≥ 0.85 表示语义保持良好

#### 7.2.3 音频扰动强度

- **linf**：L∞ 范数（最大扰动幅度）
- **l2**：L2 范数（总扰动能量）
- **snr**：信噪比（dB）
  - ≥ 30 dB：几乎不可感知
  - 20-30 dB：轻微可感知
  - < 20 dB：明显噪声

### 7.3 条件统计

- **EFR (semantic_sim ≥ 0.85)**：在语义保持良好的样本上的翻转率
- **EFR (SNR ≥ 30 dB)**：在几乎不可感知的样本上的翻转率
- **EFR (semantic_sim ≥ 0.85 AND SNR ≥ 30 dB)**：高质量样本的翻转率

## 8. 完整命令示例

### 8.1 批量实验

```bash
cd /home/xuzhenyu/OpenS2S && export PYTHONPATH=/home/xuzhenyu/OpenS2S:$PYTHONPATH && python emotion_editing_v6/attack/experiments/sad2happy_batch_experiment.py \
    --omnispeech-path models/OpenS2S_ckpt \
    --checkpoint emotion_editing_v6/checkpoints/sad_happy_classifier_test.pt \
    --data-root /data3/xuzhenyu/OpenS2S/data/en_query_wav/ \
    --output-dir /data3/xuzhenyu/OpenS2S/exp/sad2happy_batch_v1/ \
    --prompt "Please describe the content and emotions of the audio." \
    --epsilon 0.002 \
    --steps 30 \
    --lambda-emo 1.0 \
    --lambda-sem 1e-2 \
    --lambda-per 1e-4 \
    --device cuda:0 \
    --seed 2025
```

### 8.2 结果评估

```bash
cd /home/xuzhenyu/OpenS2S && export PYTHONPATH=/home/xuzhenyu/OpenS2S:$PYTHONPATH && python emotion_editing_v6/attack/experiments/evaluate_sad2happy_results.py \
    --results-csv /data3/xuzhenyu/OpenS2S/exp/sad2happy_batch_v1/results.csv \
    --output-dir /data3/xuzhenyu/OpenS2S/exp/sad2happy_batch_v1/eval \
    --device cuda:0
```

## 9. 参数总结表

| 参数 | 数值 | 单位/类型 | 说明 |
|------|------|-----------|------|
| **数据设置** |
| `seed` | 2025 | int | 随机种子（数据划分） |
| `train_ratio` | 0.8 | float | 训练集比例 |
| `test_ratio` | 0.2 | float | 测试集比例 |
| **攻击参数** |
| `epsilon` | 0.002 | float | 扰动上界（L∞） |
| `steps` | 30 | int | PGD 迭代步数 |
| `alpha` | 0.0002 | float | 步长（epsilon/10） |
| `lambda_emo` | 1.0 | float | 情绪损失权重 |
| `lambda_sem` | 0.01 | float | 语义损失权重 |
| `lambda_per` | 0.0001 | float | 感知损失权重 |
| **模型设置** |
| `target_layers` | ['layer_06', 'layer_16', 'layer_25'] | list | 特征提取层 |
| `svd_rank` | 20 | int | SVD 降维维度 |
| **生成设置** |
| `max_new_tokens` | 512 | int | 最大生成 token 数 |
| `temperature` | 0.85 | float | 采样温度 |
| `top_p` | 0.92 | float | 核采样参数 |
| **设备** |
| `device` | cuda:0 | str | GPU 设备 |

## 10. 实验设计原理

### 10.1 为什么选择这些参数？

1. **epsilon = 0.002**：
   - 足够小以保持不可感知性
   - 足够大以产生有效的情绪翻转
   - 经验值，平衡攻击效果和隐蔽性

2. **steps = 30**：
   - 足够的迭代次数以收敛
   - 不会过度计算（30 步通常足够）

3. **alpha = epsilon / 10**：
   - 标准 PGD 设置
   - 确保每步更新不会过大

4. **lambda_emo = 1.0**：
   - 主要目标（情绪翻转）的权重最大

5. **lambda_sem = 0.01**：
   - 次要目标（语义保持），权重较小但不可忽略

6. **lambda_per = 0.0001**：
   - 辅助约束（感知质量），权重最小

### 10.2 为什么使用 SVD 降维？

- **减少维度**：1280 → 20，降低计算复杂度
- **提高泛化性**：去除噪声，保留主要情绪特征
- **加速训练**：emotion classifier 训练更快

### 10.3 为什么选择这三层？

- **layer_06**：早期层，捕获低级音频特征
- **layer_16**：中间层，捕获中级语义特征
- **layer_25**：后期层，捕获高级情绪特征
- **平均**：综合不同层次的信息

## 11. 实验输出结构

```
/data3/xuzhenyu/OpenS2S/exp/sad2happy_batch_v1/
├── audio/
│   ├── clean/          # 2,395 个原始音频
│   └── adv/            # 2,395 个对抗音频
├── text/
│   ├── clean/          # 2,395 个 Clean 推理文本
│   └── adv/            # 2,395 个 Attack 推理文本
├── results.csv         # 所有样本的汇总（2,395 行）
├── config.json         # 实验配置
├── sad_list.txt        # 数据准备：所有 Sad 样本
├── sad_train.txt       # 数据准备：训练集
└── sad_test.txt        # 数据准备：测试集（实际使用）
```

## 12. 评估输出结构

```
/data3/xuzhenyu/OpenS2S/exp/sad2happy_batch_v1/eval/
├── results_eval.csv    # 详细评估结果（2,395 行）
└── stats_summary.json  # 统计摘要
```

## 13. 关键约束

✅ **必须遵守**：
- 使用独立模型评估（不依赖被攻击的 OpenS2S）
- 所有评估都是配对比较（clean vs adv）
- 不修改原始生成结果
- 记录所有模型版本和参数

❌ **禁止**：
- 使用被攻击模型作为评估器
- 修改 clean/adv 生成过程
- 评估非配对样本

## 14. 实验时间估算

假设处理 2,395 个测试样本：

- **每个样本时间**：约 2-3 秒（Clean 推理 + Attack + Attack 推理）
- **总时间**：约 1.5-2 小时（取决于 GPU 性能）
- **评估时间**：约 10-20 分钟（取决于模型加载速度）

## 15. 成功判据

实验成功的标准：

1. **情绪翻转率（EFR）**：≥ 30%（整体）
2. **条件 EFR**：在 semantic_sim ≥ 0.85 且 SNR ≥ 30 dB 的样本上，EFR ≥ 25%
3. **语义保持**：平均 semantic_sim ≥ 0.85
4. **音频质量**：平均 SNR ≥ 25 dB

这些指标表明攻击在保持语义和音频质量的前提下，成功实现了情绪翻转。


