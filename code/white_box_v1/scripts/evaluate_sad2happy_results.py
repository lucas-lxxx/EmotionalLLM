"""è¯„ä¼° sad2happy æ‰¹é‡å®žéªŒç»“æžœï¼ˆæ ¹æ® README.md è¦æ±‚ï¼‰"""

import argparse
import sys
import os
from pathlib import Path
import csv
import json
import numpy as np
import re
from tqdm import tqdm
from typing import Dict, List, Tuple

sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import torch
import soundfile as sf
from transformers import AutoTokenizer, AutoModelForSequenceClassification

# å°è¯•å¯¼å…¥ sentence_transformersï¼Œå¦‚æžœå¤±è´¥åˆ™ä½¿ç”¨ transformers çš„æ›¿ä»£æ–¹æ¡ˆ
try:
    from sentence_transformers import SentenceTransformer
    HAS_SENTENCE_TRANSFORMERS = True
except ImportError:
    HAS_SENTENCE_TRANSFORMERS = False
    print("âš ï¸  Warning: sentence_transformers not found, will use transformers-based semantic similarity")


def extract_emotion_from_text(text: str) -> Tuple[str, Dict[str, float]]:
    """
    ä»Ž OpenS2S çš„è¾“å‡ºæ–‡æœ¬ä¸­æå–æƒ…ç»ªæ ‡ç­¾
    å‡è®¾ prompt è¦æ±‚æ¨¡åž‹è¾“å‡ºéŸ³é¢‘çš„æƒ…ç»ªï¼Œæˆ‘ä»¬ä»Žæ–‡æœ¬ä¸­æŸ¥æ‰¾æƒ…ç»ªå…³é”®è¯
    
    Returns:
        predicted_emotion: æå–çš„æƒ…ç»ªæ ‡ç­¾
        emotion_scores: å„æƒ…ç»ªå…³é”®è¯çš„åŒ¹é…åˆ†æ•°ï¼ˆåŸºäºŽå…³é”®è¯å‡ºçŽ°ï¼‰
    """
    text_lower = text.lower().strip()
    
    # å®šä¹‰æƒ…ç»ªå…³é”®è¯åŠå…¶æƒé‡
    emotion_keywords = {
        'happy': ['happy', 'happiness', 'joy', 'joyful', 'cheerful', 'glad', 'pleased', 'delighted'],
        'sad': ['sad', 'sadness', 'sorrow', 'unhappy', 'depressed', 'melancholy', 'gloomy'],
        'neutral': ['neutral', 'calm', 'normal', 'flat', 'emotionless'],
        'angry': ['angry', 'anger', 'mad', 'furious', 'irritated'],
        'fear': ['fear', 'afraid', 'scared', 'frightened', 'anxious'],
        'surprise': ['surprise', 'surprised', 'shocked', 'amazed'],
        'disgust': ['disgust', 'disgusted', 'revolted']
    }
    
    # è®¡ç®—æ¯ä¸ªæƒ…ç»ªçš„åŒ¹é…åˆ†æ•°
    emotion_scores = {}
    for emotion, keywords in emotion_keywords.items():
        score = 0.0
        for keyword in keywords:
            # è®¡ç®—å…³é”®è¯å‡ºçŽ°æ¬¡æ•°ï¼ˆè€ƒè™‘è¯è¾¹ç•Œï¼‰
            pattern = r'\b' + re.escape(keyword) + r'\b'
            matches = len(re.findall(pattern, text_lower))
            score += matches * (1.0 / len(keywords))  # å½’ä¸€åŒ–æƒé‡
        
        # å¦‚æžœæƒ…ç»ªè¯å‡ºçŽ°åœ¨æ–‡æœ¬å¼€å¤´æˆ–å•ç‹¬å‡ºçŽ°ï¼Œç»™äºˆæ›´é«˜æƒé‡
        if any(text_lower.startswith(kw) for kw in keywords):
            score += 0.5
        if any(text_lower == kw for kw in keywords):
            score += 1.0
        
        emotion_scores[emotion] = score
    
    # æ‰¾åˆ°å¾—åˆ†æœ€é«˜çš„æƒ…ç»ª
    if max(emotion_scores.values()) > 0:
        predicted_emotion = max(emotion_scores, key=emotion_scores.get)
    else:
        # å¦‚æžœæ²¡æœ‰æ‰¾åˆ°æ˜Žç¡®çš„æƒ…ç»ªï¼Œè¿”å›ž neutral
        predicted_emotion = 'neutral'
        emotion_scores['neutral'] = 0.1
    
    # å½’ä¸€åŒ–åˆ†æ•°ä¸ºæ¦‚çŽ‡ï¼ˆç”¨äºŽå…¼å®¹æ€§ï¼‰
    total_score = sum(emotion_scores.values())
    if total_score > 0:
        emotion_probs = {k: v / total_score for k, v in emotion_scores.items()}
    else:
        emotion_probs = {k: 0.0 for k in emotion_scores.keys()}
        emotion_probs[predicted_emotion] = 1.0
    
    return predicted_emotion, emotion_probs


def load_semantic_model(device: str = "cuda:0"):
    """åŠ è½½è¯­ä¹‰ç›¸ä¼¼åº¦æ¨¡åž‹ï¼ˆSentence-BERT æˆ– transformers æ›¿ä»£æ–¹æ¡ˆï¼‰"""
    print("Loading semantic similarity model...")
    
    if HAS_SENTENCE_TRANSFORMERS:
        try:
            # ä½¿ç”¨ all-MiniLM-L6-v2 æˆ– all-mpnet-base-v2
            model = SentenceTransformer('all-MiniLM-L6-v2', device=device)
            print("âœ… Semantic model loaded: all-MiniLM-L6-v2 (Sentence-BERT)")
            return model
        except Exception as e:
            print(f"âš ï¸  Warning: Failed to load Sentence-BERT: {e}")
            print("   Trying alternative model...")
            try:
                model = SentenceTransformer('paraphrase-MiniLM-L6-v2', device=device)
                print("âœ… Semantic model loaded: paraphrase-MiniLM-L6-v2 (Sentence-BERT)")
                return model
            except Exception as e2:
                print(f"âš ï¸  Warning: Failed to load Sentence-BERT models: {e2}")
                print("   Falling back to transformers-based approach...")
    
    # ä½¿ç”¨ transformers çš„æ›¿ä»£æ–¹æ¡ˆ
    try:
        from transformers import AutoModel
        model_name = "sentence-transformers/all-MiniLM-L6-v2"
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModel.from_pretrained(model_name).to(device)
        model.eval()
        print(f"âœ… Semantic model loaded: {model_name} (transformers)")
        return {'tokenizer': tokenizer, 'model': model, 'device': device, 'use_transformers': True}
    except Exception as e:
        print(f"âŒ Error: Failed to load semantic model: {e}")
        print("   Please install sentence_transformers: pip install sentence-transformers")
        raise


def compute_semantic_similarity(text1: str, text2: str, model) -> float:
    """è®¡ç®—ä¸¤ä¸ªæ–‡æœ¬çš„è¯­ä¹‰ç›¸ä¼¼åº¦ï¼ˆcosine similarityï¼‰"""
    if HAS_SENTENCE_TRANSFORMERS and isinstance(model, SentenceTransformer):
        # ä½¿ç”¨ SentenceTransformer
        embeddings = model.encode([text1, text2])
        similarity = np.dot(embeddings[0], embeddings[1]) / (
            np.linalg.norm(embeddings[0]) * np.linalg.norm(embeddings[1])
        )
        return float(similarity)
    elif isinstance(model, dict) and model.get('use_transformers', False):
        # ä½¿ç”¨ transformers çš„æ›¿ä»£æ–¹æ¡ˆ
        tokenizer = model['tokenizer']
        encoder = model['model']
        device = model['device']
        
        # ç¼–ç ä¸¤ä¸ªæ–‡æœ¬
        def encode_text(text):
            inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=512, padding=True).to(device)
            with torch.no_grad():
                outputs = encoder(**inputs)
                # ä½¿ç”¨ mean pooling
                embeddings = outputs.last_hidden_state
                attention_mask = inputs['attention_mask']
                # åº”ç”¨ attention mask å¹¶å¹³å‡
                masked_embeddings = embeddings * attention_mask.unsqueeze(-1)
                sum_embeddings = masked_embeddings.sum(dim=1)
                sum_mask = attention_mask.sum(dim=1, keepdim=True).clamp(min=1e-9)
                mean_embeddings = sum_embeddings / sum_mask
                return mean_embeddings[0].cpu().numpy()
        
        emb1 = encode_text(text1)
        emb2 = encode_text(text2)
        
        similarity = np.dot(emb1, emb2) / (
            np.linalg.norm(emb1) * np.linalg.norm(emb2) + 1e-8
        )
        return float(similarity)
    else:
        raise ValueError("Unknown model type for semantic similarity")


def compute_audio_metrics(audio_clean_path: str, audio_adv_path: str) -> Tuple[float, float]:
    """
    è®¡ç®—éŸ³é¢‘æ‰°åŠ¨æŒ‡æ ‡
    
    Returns:
        l2: L2èŒƒæ•°
        snr: ä¿¡å™ªæ¯”ï¼ˆdBï¼‰
    """
    # åŠ è½½éŸ³é¢‘
    audio_clean, sr_clean = sf.read(audio_clean_path)
    audio_adv, sr_adv = sf.read(audio_adv_path)
    
    # ç¡®ä¿é‡‡æ ·çŽ‡ä¸€è‡´
    if sr_clean != sr_adv:
        raise ValueError(f"Sample rate mismatch: {sr_clean} vs {sr_adv}")
    
    # ç¡®ä¿é•¿åº¦ä¸€è‡´
    min_len = min(len(audio_clean), len(audio_adv))
    audio_clean = audio_clean[:min_len]
    audio_adv = audio_adv[:min_len]
    
    # è½¬æ¢ä¸º tensor
    audio_clean_t = torch.from_numpy(audio_clean).float()
    audio_adv_t = torch.from_numpy(audio_adv).float()
    
    # è®¡ç®—æ‰°åŠ¨
    perturbation = audio_adv_t - audio_clean_t
    
    # L2èŒƒæ•°
    l2 = torch.norm(perturbation, p=2).item()
    
    # SNR (dB)
    signal_power = torch.mean(audio_clean_t ** 2).item()
    noise_power = torch.mean(perturbation ** 2).item()
    if noise_power > 0:
        snr = 10 * np.log10(signal_power / noise_power)
    else:
        snr = float('inf')
    
    return l2, snr


def evaluate_batch_results(
    results_csv_path: str,
    output_dir: str,
    device: str = "cuda:0",
    recompute_audio_metrics: bool = False
):
    """
    è¯„ä¼°æ‰¹é‡å®žéªŒç»“æžœ
    
    Args:
        results_csv_path: results.csv è·¯å¾„
        output_dir: è¾“å‡ºç›®å½•
        device: è®¾å¤‡
        recompute_audio_metrics: æ˜¯å¦é‡æ–°è®¡ç®—éŸ³é¢‘æŒ‡æ ‡ï¼ˆå¦‚æžœresults.csvå·²æœ‰åˆ™è·³è¿‡ï¼‰
    """
    print("=" * 80)
    print("Evaluating sad2happy Batch Experiment Results")
    print("=" * 80)
    
    # è¯»å– results.csv
    print(f"\n[1/5] Reading results.csv...")
    results = []
    with open(results_csv_path, 'r', encoding='utf-8') as f:
        reader = csv.DictReader(f)
        for row in reader:
            results.append(row)
    
    print(f"  Total samples: {len(results)}")
    
    # åŠ è½½æ¨¡åž‹ï¼ˆä¸å†éœ€è¦æƒ…ç»ªåˆ†ç±»å™¨ï¼Œç›´æŽ¥ä»Žæ–‡æœ¬æå–ï¼‰
    print(f"\n[2/5] Loading evaluation models...")
    print("  Note: Using emotion extraction from OpenS2S output text (not external classifier)")
    sem_model = load_semantic_model(device)
    
    # è¯„ä¼°æ¯æ¡æ ·æœ¬
    print(f"\n[3/5] Evaluating samples...")
    eval_results = []
    
    for result in tqdm(results, desc="Evaluating"):
        sample_id = result['sample_id']
        eval_result = {
            'sample_id': sample_id,
        }
        
        # è¯»å–æ–‡æœ¬
        t_clean_path = result.get('t_clean_path', '')
        t_adv_path = result.get('t_adv_path', '')
        
        if not t_clean_path or not t_adv_path:
            print(f"  âš ï¸  Warning: Missing text paths for sample {sample_id}")
            continue
        
        try:
            # è¯»å–æ–‡æœ¬å†…å®¹
            with open(t_clean_path, 'r', encoding='utf-8') as f:
                t_clean = f.read().strip()
            with open(t_adv_path, 'r', encoding='utf-8') as f:
                t_adv = f.read().strip()
            
            # Part I: æƒ…ç»ªè¯„ä¼°ï¼ˆä»Ž OpenS2S è¾“å‡ºæ–‡æœ¬ä¸­æå–æƒ…ç»ªï¼‰
            emo_clean, probs_clean = extract_emotion_from_text(t_clean)
            emo_adv, probs_adv = extract_emotion_from_text(t_adv)
            
            # æå–æƒ…ç»ªæ¦‚çŽ‡
            # èŽ·å–æ‰€æœ‰å¯èƒ½çš„æƒ…ç»ªæ ‡ç­¾
            all_emotions = set(list(probs_clean.keys()) + list(probs_adv.keys()))
            
            # æå– Sad å’Œ Happy ç›¸å…³çš„æ¦‚çŽ‡ï¼ˆä»Žæå–çš„æƒ…ç»ªæ¦‚çŽ‡ä¸­èŽ·å–ï¼‰
            p_sad_clean = probs_clean.get('sad', 0.0)
            p_happy_clean = probs_clean.get('happy', 0.0)
            p_sad_adv = probs_adv.get('sad', 0.0)
            p_happy_adv = probs_adv.get('happy', 0.0)
            
            delta_happy = p_happy_adv - p_happy_clean
            delta_sad = p_sad_adv - p_sad_clean  # åº”è¯¥æ˜¯è´Ÿæ•°ï¼ˆsadå‡å°‘ï¼‰
            
            # åˆ¤æ–­æƒ…ç»ªç¿»è½¬ï¼šåŸºäºŽ OpenS2S è¾“å‡ºçš„éŸ³é¢‘æƒ…ç»ª
            # æˆåŠŸç¿»è½¬ï¼šclean è¾“å‡º sadï¼Œadv è¾“å‡º happy
            # æˆ–è€…ï¼šadv çš„ happy æ¦‚çŽ‡æ˜Žæ˜¾æå‡ï¼Œsad æ¦‚çŽ‡æ˜Žæ˜¾ä¸‹é™
            emotion_flip = 0
            if emo_clean.lower() in ['sad', 'sadness'] and emo_adv.lower() in ['happy', 'joy']:
                # æ˜Žç¡®çš„ç¿»è½¬ï¼šsad -> happy
                emotion_flip = 1
            elif p_happy_adv > p_sad_adv and delta_happy > 0 and delta_sad < 0:
                # æ¦‚çŽ‡æå‡ï¼šhappy å¢žåŠ ï¼Œsad å‡å°‘
                emotion_flip = 1
            elif emo_adv.lower() in ['happy', 'joy'] and delta_happy > 0.1:
                # adv è¾“å‡º happy ä¸”æ¦‚çŽ‡æœ‰æ˜Žæ˜¾æå‡
                emotion_flip = 1
            
            eval_result.update({
                'emo_clean': emo_clean,
                'emo_adv': emo_adv,
                'p_happy_clean': p_happy_clean,
                'p_happy_adv': p_happy_adv,
                'delta_happy': delta_happy,
                'emotion_flip': emotion_flip,
            })
            
            # Part II: è¯­ä¹‰ç›¸ä¼¼åº¦
            semantic_sim = compute_semantic_similarity(t_clean, t_adv, sem_model)
            eval_result['semantic_sim'] = semantic_sim
            
            # Part III: éŸ³é¢‘æ‰°åŠ¨æŒ‡æ ‡
            if recompute_audio_metrics or not result.get('l2') or not result.get('snr'):
                audio_clean_path = result.get('audio_clean_path', '')
                audio_adv_path = result.get('audio_adv_path', '')
                
                if audio_clean_path and audio_adv_path and os.path.exists(audio_clean_path) and os.path.exists(audio_adv_path):
                    try:
                        l2, snr = compute_audio_metrics(audio_clean_path, audio_adv_path)
                        eval_result['l2'] = l2
                        eval_result['snr'] = snr
                    except Exception as e:
                        print(f"  âš ï¸  Warning: Failed to compute audio metrics for {sample_id}: {e}")
                        eval_result['l2'] = result.get('l2', 0.0)
                        eval_result['snr'] = result.get('snr', 0.0)
                else:
                    eval_result['l2'] = result.get('l2', 0.0)
                    eval_result['snr'] = result.get('snr', 0.0)
            else:
                eval_result['l2'] = float(result.get('l2', 0.0))
                eval_result['snr'] = float(result.get('snr', 0.0))
            
            eval_results.append(eval_result)
            
        except Exception as e:
            print(f"  âš ï¸  Error processing sample {sample_id}: {e}")
            continue
    
    # ä¿å­˜è¯„ä¼°ç»“æžœ
    print(f"\n[4/5] Saving evaluation results...")
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    eval_csv_path = output_path / "results_eval.csv"
    if eval_results:
        fieldnames = [
            'sample_id', 'emo_clean', 'emo_adv',
            'p_happy_clean', 'p_happy_adv', 'delta_happy', 'emotion_flip',
            'semantic_sim', 'l2', 'snr'
        ]
        with open(eval_csv_path, 'w', newline='', encoding='utf-8') as f:
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(eval_results)
        print(f"  âœ… Saved: {eval_csv_path}")
    
    # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
    print(f"\n[5/5] Computing statistics...")
    stats = compute_statistics(eval_results)
    
    # ä¿å­˜ç»Ÿè®¡æ‘˜è¦
    stats_path = output_path / "stats_summary.json"
    with open(stats_path, 'w', encoding='utf-8') as f:
        json.dump(stats, f, indent=2, ensure_ascii=False)
    print(f"  âœ… Saved: {stats_path}")
    
    # æ‰“å°ç»Ÿè®¡æ‘˜è¦
    print("\n" + "=" * 80)
    print("Evaluation Statistics Summary")
    print("=" * 80)
    print_statistics(stats)
    print("=" * 80)
    
    return eval_results, stats


def compute_statistics(eval_results: List[Dict]) -> Dict:
    """è®¡ç®—ç»Ÿè®¡ä¿¡æ¯"""
    if not eval_results:
        return {}
    
    # æå–æ‰€æœ‰æŒ‡æ ‡
    emotion_flips = [r['emotion_flip'] for r in eval_results]
    delta_happies = [r['delta_happy'] for r in eval_results]
    semantic_sims = [r['semantic_sim'] for r in eval_results]
    snrs = [r['snr'] for r in eval_results if not np.isinf(r['snr'])]
    l2s = [r['l2'] for r in eval_results]
    
    # æ•´ä½“ç»Ÿè®¡
    stats = {
        'overall': {
            'total_samples': len(eval_results),
            'emotion_flip_rate': np.mean(emotion_flips),
            'delta_happy_mean': np.mean(delta_happies),
            'delta_happy_median': np.median(delta_happies),
            'semantic_sim_mean': np.mean(semantic_sims),
            'semantic_sim_median': np.median(semantic_sims),
            'snr_mean': np.mean(snrs) if snrs else None,
            'snr_median': np.median(snrs) if snrs else None,
            'l2_mean': np.mean(l2s),
            'l2_median': np.median(l2s),
        }
    }
    
    # æ¡ä»¶ç»Ÿè®¡ï¼šsemantic_sim >= 0.85
    high_sem_samples = [r for r in eval_results if r['semantic_sim'] >= 0.85]
    if high_sem_samples:
        high_sem_flips = [r['emotion_flip'] for r in high_sem_samples]
        stats['conditional_semantic_high'] = {
            'count': len(high_sem_samples),
            'emotion_flip_rate': np.mean(high_sem_flips),
            'delta_happy_mean': np.mean([r['delta_happy'] for r in high_sem_samples]),
        }
    
    # æ¡ä»¶ç»Ÿè®¡ï¼šsnr >= 30 dB
    high_snr_samples = [r for r in eval_results if not np.isinf(r['snr']) and r['snr'] >= 30.0]
    if high_snr_samples:
        high_snr_flips = [r['emotion_flip'] for r in high_snr_samples]
        stats['conditional_snr_high'] = {
            'count': len(high_snr_samples),
            'emotion_flip_rate': np.mean(high_snr_flips),
            'delta_happy_mean': np.mean([r['delta_happy'] for r in high_snr_samples]),
        }
    
    # åŒé‡æ¡ä»¶ï¼šsemantic_sim >= 0.85 AND snr >= 30 dB
    high_quality_samples = [
        r for r in eval_results
        if r['semantic_sim'] >= 0.85 and not np.isinf(r['snr']) and r['snr'] >= 30.0
    ]
    if high_quality_samples:
        high_quality_flips = [r['emotion_flip'] for r in high_quality_samples]
        stats['conditional_high_quality'] = {
            'count': len(high_quality_samples),
            'emotion_flip_rate': np.mean(high_quality_flips),
            'delta_happy_mean': np.mean([r['delta_happy'] for r in high_quality_samples]),
        }
    
    return stats


def print_statistics(stats: Dict):
    """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
    overall = stats.get('overall', {})
    print(f"\nðŸ“Š Overall Statistics:")
    print(f"  Total samples: {overall.get('total_samples', 0)}")
    print(f"  Emotion Flip Rate (EFR): {overall.get('emotion_flip_rate', 0):.2%}")
    print(f"  Mean delta_happy: {overall.get('delta_happy_mean', 0):.4f}")
    print(f"  Median delta_happy: {overall.get('delta_happy_median', 0):.4f}")
    print(f"  Mean semantic_sim: {overall.get('semantic_sim_mean', 0):.4f}")
    print(f"  Median semantic_sim: {overall.get('semantic_sim_median', 0):.4f}")
    print(f"  Mean SNR: {overall.get('snr_mean', 0):.2f} dB")
    print(f"  Median SNR: {overall.get('snr_median', 0):.2f} dB")
    
    if 'conditional_semantic_high' in stats:
        cond = stats['conditional_semantic_high']
        print(f"\nðŸ“Š Conditional Statistics (semantic_sim >= 0.85):")
        print(f"  Count: {cond['count']}")
        print(f"  EFR: {cond['emotion_flip_rate']:.2%}")
        print(f"  Mean delta_happy: {cond['delta_happy_mean']:.4f}")
    
    if 'conditional_snr_high' in stats:
        cond = stats['conditional_snr_high']
        print(f"\nðŸ“Š Conditional Statistics (SNR >= 30 dB):")
        print(f"  Count: {cond['count']}")
        print(f"  EFR: {cond['emotion_flip_rate']:.2%}")
        print(f"  Mean delta_happy: {cond['delta_happy_mean']:.4f}")
    
    if 'conditional_high_quality' in stats:
        cond = stats['conditional_high_quality']
        print(f"\nðŸ“Š Conditional Statistics (semantic_sim >= 0.85 AND SNR >= 30 dB):")
        print(f"  Count: {cond['count']}")
        print(f"  EFR: {cond['emotion_flip_rate']:.2%}")
        print(f"  Mean delta_happy: {cond['delta_happy_mean']:.4f}")


def main():
    parser = argparse.ArgumentParser(description="Evaluate sad2happy batch experiment results")
    parser.add_argument("--results-csv", required=True,
                        help="Path to results.csv from batch experiment")
    parser.add_argument("--output-dir", required=True,
                        help="Output directory for evaluation results")
    parser.add_argument("--device", default="cuda:0", help="Device")
    parser.add_argument("--recompute-audio", action="store_true",
                        help="Recompute audio metrics even if already in results.csv")
    
    args = parser.parse_args()
    
    evaluate_batch_results(
        results_csv_path=args.results_csv,
        output_dir=args.output_dir,
        device=args.device,
        recompute_audio_metrics=args.recompute_audio
    )


if __name__ == "__main__":
    main()

