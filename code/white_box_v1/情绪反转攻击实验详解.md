# 情绪反转攻击实验详解：从原理到实现

> 本文档面向科研初学者，详细讲解语音情绪反转（Sad → Happy）白盒对抗攻击实验的完整思路和实现方法。

---

## 目录
1. [研究背景与动机](#1-研究背景与动机)
2. [问题定义](#2-问题定义)
3. [核心思路](#3-核心思路)
4. [技术原理](#4-技术原理)
5. [实现架构](#5-实现架构)
6. [数学推导](#6-数学推导)
7. [代码实现解析](#7-代码实现解析)
8. [实验流程](#8-实验流程)
9. [参数调优指南](#9-参数调优指南)
10. [常见问题与调试](#10-常见问题与调试)

---

## 1. 研究背景与动机

### 1.1 什么是对抗攻击？

**对抗攻击（Adversarial Attack）** 是一种针对深度学习模型的攻击方法。攻击者在输入数据上添加精心设计的**微小扰动**，这些扰动人类几乎无法察觉，但会导致模型做出**错误的预测**。

**举个例子：**
- 假设你有一张猫的图片，深度学习模型正确识别为"猫"
- 攻击者在图片上添加一些肉眼几乎看不出的噪声
- 模型突然把这张图片识别为"狗"
- 但人类看起来，这仍然是一张猫的图片

**为什么研究对抗攻击？**
1. **安全性评估**：发现 AI 系统的脆弱性，提高鲁棒性
2. **理解模型**：通过攻击理解模型的决策机制
3. **防御研究**：知道如何攻击，才能更好地防御

### 1.2 为什么研究语音情绪的对抗攻击？

语音情绪识别在实际应用中越来越重要：
- **客服系统**：识别用户情绪，提供更好的服务
- **心理健康**：监测抑郁、焦虑等情绪状态
- **智能助手**：理解用户情绪，做出恰当回应

**潜在风险：**
如果攻击者能通过添加不可察觉的噪声来操纵情绪识别结果，可能带来严重后果：
- 伪造情绪状态，绕过心理健康监测
- 操纵客服系统的情绪分析
- 在语音认证系统中伪装情绪特征

因此，**研究这类攻击有助于构建更鲁棒的语音情绪识别系统**。

---

## 2. 问题定义

### 2.1 任务目标

**输入：** 一段带有"悲伤（Sad）"情绪的音频 x

**目标：** 生成一段对抗音频 x'，使得：
1. **情绪翻转**：OpenS2S 模型将 x' 识别为"快乐（Happy）"情绪
2. **语义保持**：x' 的语义内容（说的话）与 x 保持一致
3. **不可察觉**：x' 与 x 的差异（扰动）人耳几乎无法察觉

**数学形式：**
```
x' = x + δ
```
其中：
- x：原始音频波形
- δ：对抗扰动（需要优化）
- x'：对抗音频
- 约束：||δ||∞ ≤ ε（扰动有界，通常 ε = 0.002）

### 2.2 攻击类型：白盒攻击

**白盒攻击（White-box Attack）** 假设攻击者拥有：
- 完整的模型架构
- 模型的所有参数（权重）
- 模型的梯度信息（可以反向传播）

这是最强的攻击假设，与之相对的是**黑盒攻击**（只能查询模型输出，无法获取梯度）。

---

## 3. 核心思路

### 3.1 整体策略

这个实验采用 **基于优化的对抗攻击** 方法，核心思想是：

> 通过**梯度优化**找到一个扰动 δ，使得模型对扰动后的音频 x' = x + δ 的预测结果发生改变（从 Sad 变为 Happy），同时满足各种约束。

**类比：**
想象你在调整音频的每一个采样点的值，就像用画笔在图片上一笔一笔地涂抹。你的目标是：
1. 让模型"看错"（情绪翻转）
2. 让人"看不出"（扰动小）
3. 让内容不变（语义保持）

### 3.2 优化算法：PGD（Projected Gradient Descent）

**PGD** 是一种迭代优化算法，步骤如下：

```
1. 初始化扰动 δ（随机或零向量）
2. 重复 T 次（如 30 步）：
   a. 计算损失函数 L(x + δ)
   b. 计算梯度 ∇δ L
   c. 沿梯度方向更新 δ
   d. 投影到约束集（确保 ||δ||∞ ≤ ε）
3. 返回最终的对抗音频 x' = x + δ
```

**关键点：**
- **梯度上升**：我们要最大化损失（让模型出错），而不是最小化
- **投影（Projection）**：每次更新后，确保扰动不超过允许范围

---

## 4. 技术原理

### 4.1 损失函数设计

这是整个攻击的**核心**。我们设计了一个**多目标损失函数**，包含三个部分：

```
L_total = λ_emo × L_emo + λ_sem × L_sem + λ_per × L_per
```

#### **(1) 情绪翻转损失 L_emo**

**目标：** 让模型将音频识别为目标情绪（Happy）

**方法：** 使用**情绪分类器**预测情绪概率，通过交叉熵损失最大化目标情绪的概率。

**工作原理：**
1. 从 OpenS2S 模型的音频编码器中提取**隐藏状态（hidden states）**
   - 选择多个层（如 layer_06, layer_16, layer_25）
   - 对时间维度进行平均池化，得到固定维度的特征向量 z

2. 将 z 输入**冻结的情绪分类器**（事先训练好的线性分类器）
   - 输出：每个情绪类别的 logits
   - 计算 softmax 概率分布

3. 计算交叉熵损失：
   ```
   L_emo = CrossEntropy(logits, target_emotion_idx)
   ```
   - 目标：最小化 L_emo，即最大化 target_emotion（Happy）的概率

**为什么这样做？**
- OpenS2S 是一个生成式模型，直接从输出文本中提取情绪不够可导（discrete）
- 使用中间层的 hidden states + 分类器，提供了一个**可微分**的优化目标

#### **(2) 语义保持损失 L_sem**

**目标：** 保持音频的语义内容不变（说的话不变）

**方法：** 计算原始音频和对抗音频的 hidden states 之间的**余弦相似度**。

```
L_sem = 1 - cosine_similarity(z_adv, z_orig)
```

**解释：**
- z_orig：原始音频的 hidden states
- z_adv：对抗音频的 hidden states
- 余弦相似度越高，语义越接近
- 损失函数鼓励两者保持相似

**为什么用 hidden states 而不是文本输出？**
- 文本输出是离散的（token），不可微分
- Hidden states 是连续的，可以计算梯度

#### **(3) 感知约束损失 L_per**

**目标：** 限制扰动的大小，使其不易被人类察觉

**方法：** 结合多种音频质量指标：

```
L_per = L2_loss + 0.5 × Linf_loss + 0.3 × SNR_loss
```

**三个指标：**

1. **L2 范数**（欧几里得距离）：
   ```
   L2_loss = ||x' - x||₂ / ||x||₂
   ```
   - 衡量整体扰动能量

2. **Linf 范数**（最大绝对值）：
   ```
   Linf_loss = ||x' - x||∞ / ||x||∞
   ```
   - 确保任何单个采样点的扰动不会太大

3. **SNR（信噪比）**：
   ```
   SNR(dB) = 10 × log₁₀(signal_power / noise_power)
   SNR_loss = -SNR / 40  （归一化）
   ```
   - 鼓励高信噪比（噪声小）

### 4.2 情绪分类器的训练

**为什么需要单独训练一个情绪分类器？**

OpenS2S 是一个**语音到语音**的生成模型，它没有显式的情绪分类头。为了在攻击过程中获得可导的情绪预测，我们需要：

1. **数据准备**：
   - 使用 ESD（Emotional Speech Database）数据集
   - 包含多种情绪（Sad, Happy, Angry, Neutral 等）

2. **特征提取**：
   - 将音频输入 OpenS2S 的音频编码器
   - 提取指定层的 hidden states（如 layer_06, layer_16, layer_25）
   - 平均池化后拼接/平均，得到特征向量 z

3. **降维（可选）**：
   - 使用 SVD（奇异值分解）降维到 R 维（如 20 维）
   - 减少过拟合，提高泛化性

4. **训练线性分类器**：
   - 输入：z ∈ ℝᴿ
   - 输出：logits ∈ ℝᶜ（C 为情绪类别数）
   - 损失：交叉熵
   - 优化器：Adam

5. **冻结分类器**：
   - 训练完成后，冻结所有参数
   - 在攻击时，只优化音频扰动，不更新分类器

**关键代码：** `scripts/train_sad_happy_classifier.py`

### 4.3 PGD 优化细节

**算法流程：**

```python
# 伪代码
δ = random_uniform(-ε, ε)  # 随机初始化
for step in range(T):
    x' = x + δ
    δ.requires_grad = True

    # 前向传播
    loss, metrics = objective_fn(x', x)

    # 反向传播
    loss.backward()
    grad = δ.grad

    # 梯度上升（最大化损失）
    δ = δ + α × sign(grad)

    # 投影到约束集
    δ = clip(δ, -ε, ε)

    δ = δ.detach()
```

**参数说明：**
- **ε (epsilon)**：扰动上界，通常 0.002
- **α (alpha)**：步长，通常 ε / 10
- **T (steps)**：迭代次数，通常 30-50 步
- **sign(grad)**：梯度的符号，用于 Linf 范数约束

**为什么用 sign(grad)？**
- 这是 **FGSM（Fast Gradient Sign Method）** 的思想
- 对于 Linf 约束，沿符号方向更新最高效
- 每次更新都让扰动尽可能接近边界

---

## 5. 实现架构

### 5.1 项目结构

```
OpenS2S_white_box/
├── attack/                          # 攻击核心代码
│   ├── objectives.py                # 目标函数（损失函数）
│   ├── optimizers/pgd.py            # PGD 优化器
│   └── transforms.py                # EOT 变换（可选）
│
├── utils/                           # 工具模块
│   ├── emotion_classifier.py       # 情绪分类器
│   ├── esd_data_loader.py          # ESD 数据加载器
│
├── scripts/                         # 实验脚本
│   ├── train_sad_happy_classifier.py  # 训练分类器
│   ├── sad2happy_batch_data_prep.py   # 数据准备
│   ├── sad2happy_batch_experiment.py  # 主实验脚本
│   └── evaluate_sad2happy_results.py  # 评估脚本
│
├── constants.py                     # 常量定义
├── utils_audio.py                   # 音频工具函数
└── requirements.txt                 # 依赖
```

### 5.2 核心类与函数

#### **EmotionAttackObjective** (`attack/objectives.py`)

**作用：** 定义攻击目标函数，计算损失和梯度

**核心方法：**
```python
class EmotionAttackObjective:
    def __init__(self, model, tokenizer, audio_extractor,
                 emotion_classifier, target_emotion, ...):
        # 初始化
        # 注册 hooks 来提取 hidden states

    def compute_loss(self, waveform_adv, waveform_orig, text, compute_grad):
        # 计算总损失
        loss_emo = self._compute_emotion_text_loss(...)
        loss_sem = self._compute_semantic_loss(...)
        loss_per = self._compute_perceptual_loss(...)
        return total_loss, metrics

    def _extract_hidden_states(self):
        # 从 audio encoder 提取 hidden states
```

**工作流程：**
1. 注册 PyTorch **forward hooks** 到音频编码器的指定层
2. 前向传播时，自动缓存 hidden states
3. 提取并平均池化，得到特征向量 z
4. 计算三个损失项
5. 返回总损失（用于反向传播）

#### **PGD** (`attack/optimizers/pgd.py`)

**作用：** 执行 PGD 优化算法

**核心方法：**
```python
class PGD:
    def __init__(self, objective_fn, eps, alpha, steps, ...):
        # 配置优化参数

    def attack(self, waveform_orig, eot_transform=None):
        # 执行 PGD 攻击
        # 返回对抗音频 + 指标历史
```

**关键步骤：**
1. 随机初始化扰动
2. 迭代优化（梯度上升 + 投影）
3. 记录每步的损失和指标
4. 返回最终对抗样本

#### **FrozenEmotionClassifier** (`utils/emotion_classifier.py`)

**作用：** 情绪分类器（冻结参数）

```python
class FrozenEmotionClassifier(nn.Module):
    def __init__(self, input_dim, num_emotions):
        self.linear = nn.Linear(input_dim, num_emotions)

    def forward(self, z):
        return self.linear(z)  # [B, R] -> [B, C]
```

**训练函数：**
```python
def train_emotion_classifier(Z_train, y_train, epochs, lr):
    # 训练线性分类器
    # 使用交叉熵损失 + Adam 优化器
```

---

## 6. 数学推导

### 6.1 优化问题的形式化

**原始优化问题：**
```
minimize   -L_emo(x + δ) + λ_sem × L_sem(x + δ, x) + λ_per × L_per(δ)
subject to ||δ||∞ ≤ ε
```

**等价于：**
```
maximize   L_emo(x + δ)
subject to ||δ||∞ ≤ ε
           L_sem ≤ threshold_sem
           L_per ≤ threshold_per
```

**实际实现：** 使用**软约束**（通过权重 λ 控制），而不是硬约束。

### 6.2 梯度计算

对于音频波形 x'，计算损失对 x' 的梯度：

```
∇_{x'} L_total = λ_emo × ∇_{x'} L_emo
               + λ_sem × ∇_{x'} L_sem
               + λ_per × ∇_{x'} L_per
```

**情绪损失的梯度链：**
```
∇_{x'} L_emo = ∇_{x'} z × ∇_z logits × ∇_{logits} CE_loss
```
其中：
- x' → audio encoder → hidden states → z（需要 hooks）
- z → emotion classifier → logits
- logits → CrossEntropy → loss

**PyTorch 自动微分：**
所有梯度由 PyTorch 自动计算，只需调用 `loss.backward()`。

### 6.3 投影操作

**Linf 投影：**
```python
δ = clip(δ, -ε, ε)  # 逐元素截断
```

**L2 投影：**
```python
if ||δ||₂ > ε:
    δ = δ / ||δ||₂ × ε  # 归一化到球面
```

本实验使用 **Linf 投影**（更常见于音频攻击）。

---

## 7. 代码实现解析

### 7.1 主实验流程 (`scripts/sad2happy_batch_experiment.py`)

**完整流程：**

```python
# 1. 准备数据
prepare_data(data_root, output_dir, seed)
# - 扫描 Sad 音频文件
# - 80/20 划分训练集/测试集

# 2. 加载模型
model, tokenizer = load_model(omnispeech_path, device)
audio_extractor = load_audio_extractor(omnispeech_path)

# 3. 加载情绪分类器
checkpoint = torch.load(checkpoint_path)
emotion_classifier = FrozenEmotionClassifier(input_dim, num_emotions)
emotion_classifier.load_state_dict(checkpoint['classifier'])
emotion_classifier.freeze()

# 4. 对每个测试样本：
for audio_path in test_list:
    # 4.1 Clean 推理（原始音频）
    clean_text = clean_inference(model, tokenizer, audio_extractor,
                                   audio_path, prompt, device)

    # 4.2 运行攻击
    waveform_adv, metrics, attack_time = run_attack(
        model, tokenizer, audio_extractor,
        audio_path, prompt,
        target_emotion="Happy",
        source_emotion="Sad",
        emotion_classifier=emotion_classifier,
        epsilon=0.002, steps=30, ...
    )

    # 4.3 保存对抗音频
    sf.write(output_audio_path, waveform_adv.cpu().numpy(), sample_rate)

    # 4.4 Attack 推理（对抗音频）
    attack_text = attack_inference(model, tokenizer, audio_extractor,
                                     output_audio_path, prompt, device)

    # 4.5 记录结果
    results.append({
        'sample_id': sample_id,
        'clean_text': clean_text,
        'attack_text': attack_text,
        'linf': metrics['linf'],
        'snr': metrics['snr'],
        ...
    })

# 5. 保存结果
save_results_csv(results, output_dir)
```

### 7.2 攻击函数 (`run_attack`)

**核心代码：**

```python
def run_attack(model, tokenizer, audio_extractor, audio_path,
               prompt, target_emotion, source_emotion,
               emotion_classifier, emotion_label_to_idx,
               epsilon=0.002, steps=30, alpha=None,
               lambda_emo=1.0, lambda_sem=1e-2, lambda_per=1e-4,
               device="cuda:0"):

    # 1. 加载原始音频
    waveform_orig, sample_rate = load_waveform(audio_path)
    waveform_orig = waveform_orig.to(device)

    # 2. 创建目标函数
    objective = EmotionAttackObjective(
        model=model,
        tokenizer=tokenizer,
        audio_extractor=audio_extractor,
        target_emotion=target_emotion,
        source_emotion=source_emotion,
        emotion_classifier=emotion_classifier,
        emotion_label_to_idx=emotion_label_to_idx,
        target_layers=['layer_06', 'layer_16', 'layer_25'],
        weight_emo_text=lambda_emo,
        weight_sem=lambda_sem,
        weight_per=lambda_per,
        device=device
    )

    # 3. 创建 PGD 优化器
    attacker = PGD(
        objective_fn=lambda w, w_orig, compute_grad=True:
            objective.compute_loss(w, w_orig, prompt, compute_grad),
        eps=epsilon,
        alpha=alpha if alpha else epsilon / 10.0,
        steps=steps,
        eot_k=1,  # 不使用 EOT
        norm="Linf",
        device=device
    )

    # 4. 执行攻击
    waveform_adv, metrics_history = attacker.attack(
        waveform_orig=waveform_orig,
        eot_transform=None
    )

    # 5. 计算最终指标
    perturbation = waveform_adv - waveform_orig
    linf = torch.max(torch.abs(perturbation)).item()
    l2 = torch.norm(perturbation, p=2).item()
    snr_db = 10 * np.log10(signal_power / noise_power)

    return waveform_adv, final_metrics, attack_time, sample_rate
```

### 7.3 目标函数的计算 (`EmotionAttackObjective.compute_loss`)

**流程：**

```python
def compute_loss(self, waveform_adv, waveform_orig, text, compute_grad):
    # 1. 计算情绪翻转损失
    loss_emo, emo_metrics = self._compute_emotion_text_loss(
        waveform_adv, text, compute_grad
    )
    # 内部流程：
    # - 提取音频特征（WhisperFeatureExtractor）
    # - 准备文本 prompt（加入音频 token）
    # - 前向传播 OpenS2S 模型
    # - 通过 hooks 提取 hidden states
    # - 输入 emotion classifier 得到 logits
    # - 计算交叉熵损失

    # 2. 计算语义保持损失
    loss_sem, sem_metrics = self._compute_semantic_loss(
        waveform_adv, waveform_orig, text, compute_grad
    )
    # 内部流程：
    # - 如果是第一次，先缓存原始音频的 hidden states
    # - 提取当前对抗音频的 hidden states
    # - 计算余弦相似度
    # - 损失 = 1 - cosine_sim

    # 3. 计算感知约束损失
    loss_per, per_metrics = self._compute_perceptual_loss(
        waveform_adv, waveform_orig
    )
    # 内部流程：
    # - 计算 L2 范数、Linf 范数
    # - 计算 SNR（信噪比）
    # - 组合三个指标

    # 4. 总损失
    total_loss = (
        lambda_emo * loss_emo +
        lambda_sem * loss_sem +
        lambda_per * loss_per
    )

    return total_loss, metrics
```

### 7.4 PGD 优化循环 (`PGD.attack`)

**详细代码：**

```python
def attack(self, waveform_orig, eot_transform=None):
    # 1. 随机初始化扰动
    if self.random_start:
        delta = torch.empty_like(waveform_orig).uniform_(-self.eps, self.eps)
    else:
        delta = torch.zeros_like(waveform_orig)

    waveform_adv = waveform_orig + delta
    metrics_history = []

    # 2. 迭代优化
    for step in range(self.steps):
        # 2.1 启用梯度
        waveform_adv = waveform_adv.detach().requires_grad_(True)

        # 2.2 计算损失
        loss, metrics = self.objective_fn(
            waveform_adv, waveform_orig, compute_grad=True
        )

        # 2.3 反向传播
        loss.backward()
        grad = waveform_adv.grad

        # 2.4 梯度上升（Linf 范数）
        with torch.no_grad():
            waveform_adv = waveform_adv + self.alpha * torch.sign(grad)

            # 2.5 投影到约束集
            delta = waveform_adv - waveform_orig
            delta = torch.clamp(delta, -self.eps, self.eps)
            waveform_adv = waveform_orig + delta

        # 2.6 记录指标
        metrics_history.append(metrics)

        # 2.7 定期打印
        if (step + 1) % 10 == 0:
            print(f"Step {step+1}/{self.steps}: "
                  f"loss={metrics['total_loss']:.6f}, "
                  f"emo_prob_target={metrics['emo_prob_target']:.4f}, ...")

    return waveform_adv, metrics_history
```

---

## 8. 实验流程

### 8.1 环境配置

**1. 安装 OpenS2S**
```bash
# 按照 OpenS2S 官方文档安装
git clone https://github.com/xxxxx/OpenS2S.git
cd OpenS2S
pip install -e .
```

**2. 设置环境变量**
```bash
export PYTHONPATH=/path/to/OpenS2S:$PYTHONPATH
```

**3. 安装依赖**
```bash
cd /path/to/OpenS2S_white_box
pip install -r requirements.txt
```

**依赖包：**
- PyTorch >= 2.0（GPU 版本）
- torchaudio
- transformers
- soundfile
- tqdm
- numpy, scipy
- scikit-learn（用于 SVD）

### 8.2 步骤 1：训练情绪分类器

**命令：**
```bash
python scripts/train_sad_happy_classifier.py \
    --omnispeech-path /path/to/OpenS2S/models/OpenS2S_ckpt \
    --output checkpoints/sad_happy_classifier.pt \
    --emotions Sad Happy \
    --split train \
    --max-samples-per-emotion 100 \
    --max-speakers 3 \
    --svd-rank 20 \
    --epochs 50 \
    --device cuda:0
```

**参数说明：**
- `--omnispeech-path`: OpenS2S 模型路径
- `--output`: 分类器保存路径
- `--emotions`: 情绪类别（Sad, Happy）
- `--max-samples-per-emotion`: 每个情绪的最大样本数
- `--svd-rank`: SVD 降维维度（20 维）
- `--epochs`: 训练轮数

**输出：**
- `sad_happy_classifier.pt`: 包含分类器权重、SVD 参数、标签映射

### 8.3 步骤 2：准备数据

**命令：**
```bash
python scripts/sad2happy_batch_data_prep.py \
    --data-root /path/to/audio/data \
    --output-dir /path/to/output \
    --seed 2025
```

**数据格式要求：**
```
data_root/
├── Sad/
│   ├── adult/
│   │   ├── female/
│   │   │   └── *.wav
│   │   └── male/
│   │       └── *.wav
│   └── child/
└── Happy/
    └── ...
```

**输出：**
- `sad_train.txt`: 训练集文件列表（80%）
- `sad_test.txt`: 测试集文件列表（20%）

### 8.4 步骤 3：运行批量攻击实验

**命令：**
```bash
python scripts/sad2happy_batch_experiment.py \
    --omnispeech-path /path/to/OpenS2S/models/OpenS2S_ckpt \
    --checkpoint /path/to/sad_happy_classifier.pt \
    --data-root /path/to/audio/data \
    --output-dir /path/to/output \
    --prompt "What is the emotion of this audio? Please answer with only the emotion label (e.g., happy, sad, neutral)." \
    --epsilon 0.002 \
    --steps 30 \
    --lambda-emo 1.0 \
    --lambda-sem 1e-2 \
    --lambda-per 1e-4 \
    --device cuda:0 \
    --seed 2025
```

**重要参数：**
- `--epsilon`: 扰动上界（Linf 范数），默认 0.002
- `--steps`: PGD 迭代步数，默认 30
- `--lambda-emo`: 情绪损失权重，默认 1.0
- `--lambda-sem`: 语义损失权重，默认 1e-2
- `--lambda-per`: 感知损失权重，默认 1e-4

**输出文件：**
```
output_dir/
├── results.csv                 # 每个样本的详细结果
├── config.json                 # 实验配置
├── audio/
│   ├── clean/                  # 原始音频（软链接）
│   └── adv/                    # 对抗音频
└── text/
    ├── clean/                  # Clean 推理文本
    └── adv/                    # Attack 推理文本
```

**results.csv 格式：**
```csv
sample_id,clean_text,attack_text,linf,l2,snr,attack_time,...
sample001,sad emotion,happy emotion,0.0019,12.34,35.6,15.2,...
```

### 8.5 步骤 4：评估结果

**命令：**
```bash
python scripts/evaluate_sad2happy_results.py \
    --results-csv /path/to/output/results.csv \
    --output-dir /path/to/output/eval \
    --device cuda:0
```

**评估指标：**
1. **攻击成功率（ASR）**：对抗音频被识别为 Happy 的比例
2. **语义保持率**：语义相似度 > 阈值的比例
3. **平均扰动指标**：Linf, L2, SNR
4. **攻击时间**：平均每个样本的攻击时间

**输出：**
- `results_eval.csv`: 包含所有评估指标
- `stats_summary.json`: 统计摘要

---

## 9. 参数调优指南

### 9.1 核心参数的作用

| 参数 | 作用 | 典型值 | 调优建议 |
|------|------|--------|----------|
| `epsilon` | 扰动上界 | 0.001 ~ 0.005 | 越大攻击越强，但越容易被察觉 |
| `steps` | 迭代次数 | 20 ~ 50 | 越多收敛越好，但时间越长 |
| `alpha` | 步长 | ε / 10 | 通常设为 ε 的 1/10 |
| `lambda_emo` | 情绪损失权重 | 0.5 ~ 2.0 | 控制情绪翻转的强度 |
| `lambda_sem` | 语义损失权重 | 1e-3 ~ 1e-1 | 越大语义保持越好 |
| `lambda_per` | 感知损失权重 | 1e-5 ~ 1e-3 | 越大扰动越小 |

### 9.2 调优策略

**场景 1：攻击成功率低**
- 增大 `epsilon`（如 0.002 → 0.003）
- 增大 `lambda_emo`（如 1.0 → 1.5）
- 减小 `lambda_sem` 和 `lambda_per`
- 增加 `steps`（如 30 → 50）

**场景 2：扰动太明显**
- 减小 `epsilon`（如 0.002 → 0.001）
- 增大 `lambda_per`（如 1e-4 → 1e-3）
- 减小 `steps`（避免过度优化）

**场景 3：语义丢失**
- 增大 `lambda_sem`（如 1e-2 → 5e-2）
- 检查 prompt 是否合理
- 降低 `lambda_emo`

**场景 4：优化不收敛**
- 检查梯度是否为零（可能计算图断裂）
- 减小 `alpha`（步长太大）
- 检查情绪分类器是否正确加载

### 9.3 常见问题

**Q1: 梯度为零，优化无效？**
- **原因**：计算图断裂（detach 过早，hooks 未触发）
- **解决**：检查 `EmotionAttackObjective` 中的 hooks 注册，确保 `compute_grad=True`

**Q2: 攻击时间过长？**
- **原因**：模型推理慢，迭代次数多
- **解决**：减少 `steps`，使用更小的模型，批量处理

**Q3: 情绪翻转概率不增长？**
- **原因**：分类器训练不好，或特征提取层选择不当
- **解决**：重新训练分类器，尝试不同的 `target_layers`

---

## 10. 常见问题与调试

### 10.1 环境问题

**Q: `ModuleNotFoundError: No module named 'src'`**
```bash
# 解决：设置 PYTHONPATH
export PYTHONPATH=/path/to/OpenS2S:$PYTHONPATH
```

**Q: CUDA out of memory**
```bash
# 解决：减少 batch size，使用更小的模型，或增加 GPU 显存
```

### 10.2 数据问题

**Q: 找不到音频文件**
- 检查 `--data-root` 路径是否正确
- 确保目录结构符合 `{emotion}/{age}/{gender}/*.wav`

**Q: 音频格式不支持**
- 使用 `soundfile` 支持的格式（WAV, FLAC 等）
- 转换格式：`ffmpeg -i input.mp3 output.wav`

### 10.3 模型问题

**Q: OpenS2S 模型加载失败**
- 检查 `--omnispeech-path` 是否指向正确的 checkpoint 目录
- 确保目录包含 `config.json`, `pytorch_model.bin` 等文件

**Q: 情绪分类器加载失败**
- 检查 checkpoint 文件完整性
- 确保训练时的 `input_dim` 和 `num_emotions` 一致

### 10.4 调试技巧

**1. 打印中间变量**
```python
# 在 EmotionAttackObjective.compute_loss 中
print(f"z.shape: {z.shape}")
print(f"logits: {logits}")
print(f"emo_prob_target: {emo_prob_target}")
```

**2. 可视化损失曲线**
```python
import matplotlib.pyplot as plt
losses = [m['total_loss'] for m in metrics_history]
plt.plot(losses)
plt.xlabel('Step')
plt.ylabel('Loss')
plt.savefig('loss_curve.png')
```

**3. 检查梯度**
```python
# 在 PGD.attack 中
print(f"grad_norm: {torch.norm(grad).item()}")
if grad_norm < 1e-6:
    print("Warning: Gradient is too small!")
```

**4. 音频质量检查**
```python
# 播放对抗音频，听是否与原始音频相似
import sounddevice as sd
sd.play(waveform_adv.cpu().numpy(), samplerate=16000)
```

---

## 总结

本实验通过**基于优化的白盒对抗攻击**，实现了语音情绪的翻转（Sad → Happy）。核心技术包括：

1. **PGD 优化算法**：迭代优化扰动，同时满足多个约束
2. **多目标损失函数**：平衡情绪翻转、语义保持、感知约束
3. **情绪分类器**：提供可导的情绪预测目标
4. **Hidden states 提取**：使用 PyTorch hooks 从模型中间层提取特征

通过理解这些原理和实现细节，您可以：
- 复现实验结果
- 调整参数以适应不同场景
- 扩展到其他情绪对（如 Angry → Neutral）
- 设计更强的攻击或防御方法

**下一步研究方向：**
- **黑盒攻击**：仅通过查询模型输出进行攻击
- **物理攻击**：在真实环境中播放对抗音频
- **防御方法**：对抗训练、输入预处理、检测机制
- **可迁移性**：对抗样本对其他模型的攻击效果

祝您科研顺利！🚀
